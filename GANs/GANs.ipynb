{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting some hyperparameters\n",
    "batchSize = 64 # We set the size of the batch.\n",
    "imageSize = 64 # We set the size of the generated images (64x64).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the transformations\n",
    "transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dataset = dset.CIFAR10(root = './data_for_gan', download = True, transform = transform) # We download the training set in the ./data folder and we apply the previous transformations on each image.\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) # We use dataLoader to get the images of the training set batch by batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the generator\n",
    "\n",
    "class G(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G (\n",
       "  (main): Sequential (\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): ReLU (inplace)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): ReLU (inplace)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (8): ReLU (inplace)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (11): ReLU (inplace)\n",
       "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh ()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the generator\n",
    "netG = G()\n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the discriminator\n",
    "\n",
    "class D(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D (\n",
       "  (main): Sequential (\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU (0.2, inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (4): LeakyReLU (0.2, inplace)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (7): LeakyReLU (0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (10): LeakyReLU (0.2, inplace)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid ()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the discriminator\n",
    "netD = D()\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training the DCGANs\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/782] Loss_D: 2.2372 Loss_G: 5.4773\n",
      "[0/25][1/782] Loss_D: 1.1970 Loss_G: 6.4851\n",
      "[0/25][2/782] Loss_D: 0.8306 Loss_G: 6.0550\n",
      "[0/25][3/782] Loss_D: 0.9350 Loss_G: 6.3628\n",
      "[0/25][4/782] Loss_D: 0.9163 Loss_G: 6.6590\n",
      "[0/25][5/782] Loss_D: 0.7395 Loss_G: 7.3583\n",
      "[0/25][6/782] Loss_D: 0.5526 Loss_G: 8.3548\n",
      "[0/25][7/782] Loss_D: 0.7025 Loss_G: 7.6987\n",
      "[0/25][8/782] Loss_D: 1.1653 Loss_G: 9.2828\n",
      "[0/25][9/782] Loss_D: 0.5147 Loss_G: 7.7485\n",
      "[0/25][10/782] Loss_D: 0.9810 Loss_G: 11.3683\n",
      "[0/25][11/782] Loss_D: 0.4143 Loss_G: 8.3494\n",
      "[0/25][12/782] Loss_D: 1.1858 Loss_G: 12.7913\n",
      "[0/25][13/782] Loss_D: 0.2679 Loss_G: 10.0830\n",
      "[0/25][14/782] Loss_D: 0.5461 Loss_G: 9.7823\n",
      "[0/25][15/782] Loss_D: 0.6714 Loss_G: 12.1816\n",
      "[0/25][16/782] Loss_D: 0.5695 Loss_G: 9.1460\n",
      "[0/25][17/782] Loss_D: 0.8320 Loss_G: 14.3816\n",
      "[0/25][18/782] Loss_D: 0.2426 Loss_G: 11.4293\n",
      "[0/25][19/782] Loss_D: 0.3607 Loss_G: 8.0039\n",
      "[0/25][20/782] Loss_D: 1.3295 Loss_G: 17.2308\n",
      "[0/25][21/782] Loss_D: 0.5859 Loss_G: 15.9229\n",
      "[0/25][22/782] Loss_D: 0.1200 Loss_G: 9.2715\n",
      "[0/25][23/782] Loss_D: 1.5682 Loss_G: 17.0127\n",
      "[0/25][24/782] Loss_D: 0.3804 Loss_G: 15.9281\n",
      "[0/25][25/782] Loss_D: 0.3075 Loss_G: 9.1099\n",
      "[0/25][26/782] Loss_D: 1.4951 Loss_G: 16.7716\n",
      "[0/25][27/782] Loss_D: 0.1962 Loss_G: 17.1698\n",
      "[0/25][28/782] Loss_D: 0.2858 Loss_G: 12.4140\n",
      "[0/25][29/782] Loss_D: 0.0995 Loss_G: 5.9572\n",
      "[0/25][30/782] Loss_D: 2.3250 Loss_G: 19.5426\n",
      "[0/25][31/782] Loss_D: 0.4384 Loss_G: 21.7312\n",
      "[0/25][32/782] Loss_D: 0.1217 Loss_G: 18.7883\n",
      "[0/25][33/782] Loss_D: 0.1662 Loss_G: 11.9373\n",
      "[0/25][34/782] Loss_D: 0.1327 Loss_G: 5.0546\n",
      "[0/25][35/782] Loss_D: 3.0900 Loss_G: 20.8862\n",
      "[0/25][36/782] Loss_D: 0.2250 Loss_G: 23.4767\n",
      "[0/25][37/782] Loss_D: 0.2132 Loss_G: 20.8273\n",
      "[0/25][38/782] Loss_D: 0.2584 Loss_G: 13.3280\n",
      "[0/25][39/782] Loss_D: 0.1078 Loss_G: 5.2972\n",
      "[0/25][40/782] Loss_D: 3.3585 Loss_G: 22.2718\n",
      "[0/25][41/782] Loss_D: 0.7216 Loss_G: 25.5760\n",
      "[0/25][42/782] Loss_D: 0.2085 Loss_G: 24.5041\n",
      "[0/25][43/782] Loss_D: 0.1391 Loss_G: 19.9620\n",
      "[0/25][44/782] Loss_D: 0.0933 Loss_G: 12.0061\n",
      "[0/25][45/782] Loss_D: 0.0405 Loss_G: 4.7584\n",
      "[0/25][46/782] Loss_D: 2.5425 Loss_G: 21.4011\n",
      "[0/25][47/782] Loss_D: 0.7897 Loss_G: 24.0014\n",
      "[0/25][48/782] Loss_D: 0.3367 Loss_G: 22.2504\n",
      "[0/25][49/782] Loss_D: 0.2189 Loss_G: 17.4506\n",
      "[0/25][50/782] Loss_D: 0.0510 Loss_G: 11.4266\n",
      "[0/25][51/782] Loss_D: 0.0689 Loss_G: 4.6170\n",
      "[0/25][52/782] Loss_D: 1.8695 Loss_G: 19.8380\n",
      "[0/25][53/782] Loss_D: 0.1397 Loss_G: 23.6001\n",
      "[0/25][54/782] Loss_D: 0.7798 Loss_G: 21.8103\n",
      "[0/25][55/782] Loss_D: 0.5281 Loss_G: 16.9253\n",
      "[0/25][56/782] Loss_D: 0.0746 Loss_G: 9.8202\n",
      "[0/25][57/782] Loss_D: 0.2479 Loss_G: 6.6648\n",
      "[0/25][58/782] Loss_D: 0.3433 Loss_G: 11.9423\n",
      "[0/25][59/782] Loss_D: 0.0834 Loss_G: 11.4229\n",
      "[0/25][60/782] Loss_D: 0.1927 Loss_G: 7.1786\n",
      "[0/25][61/782] Loss_D: 0.2710 Loss_G: 7.5028\n",
      "[0/25][62/782] Loss_D: 0.3723 Loss_G: 11.0379\n",
      "[0/25][63/782] Loss_D: 0.2002 Loss_G: 9.1574\n",
      "[0/25][64/782] Loss_D: 0.3613 Loss_G: 8.4240\n",
      "[0/25][65/782] Loss_D: 0.6958 Loss_G: 13.0009\n",
      "[0/25][66/782] Loss_D: 0.2739 Loss_G: 11.2574\n",
      "[0/25][67/782] Loss_D: 0.0842 Loss_G: 6.8081\n",
      "[0/25][68/782] Loss_D: 0.5367 Loss_G: 17.9711\n",
      "[0/25][69/782] Loss_D: 0.2006 Loss_G: 19.0906\n",
      "[0/25][70/782] Loss_D: 0.2845 Loss_G: 14.5582\n",
      "[0/25][71/782] Loss_D: 0.2847 Loss_G: 8.4308\n",
      "[0/25][72/782] Loss_D: 0.3701 Loss_G: 8.1679\n",
      "[0/25][73/782] Loss_D: 0.2501 Loss_G: 10.9909\n",
      "[0/25][74/782] Loss_D: 0.0908 Loss_G: 8.9408\n",
      "[0/25][75/782] Loss_D: 0.2218 Loss_G: 8.7988\n",
      "[0/25][76/782] Loss_D: 0.1597 Loss_G: 7.2082\n",
      "[0/25][77/782] Loss_D: 0.1303 Loss_G: 6.3602\n",
      "[0/25][78/782] Loss_D: 0.2609 Loss_G: 9.2091\n",
      "[0/25][79/782] Loss_D: 0.2345 Loss_G: 7.6982\n",
      "[0/25][80/782] Loss_D: 0.2409 Loss_G: 6.3320\n",
      "[0/25][81/782] Loss_D: 0.1628 Loss_G: 6.8299\n",
      "[0/25][82/782] Loss_D: 0.2287 Loss_G: 6.7310\n",
      "[0/25][83/782] Loss_D: 0.1638 Loss_G: 7.1499\n",
      "[0/25][84/782] Loss_D: 0.2387 Loss_G: 5.9148\n",
      "[0/25][85/782] Loss_D: 0.4709 Loss_G: 7.1132\n",
      "[0/25][86/782] Loss_D: 0.5055 Loss_G: 3.6286\n",
      "[0/25][87/782] Loss_D: 1.0630 Loss_G: 13.6431\n",
      "[0/25][88/782] Loss_D: 1.5438 Loss_G: 7.9473\n",
      "[0/25][89/782] Loss_D: 0.2986 Loss_G: 6.6950\n",
      "[0/25][90/782] Loss_D: 0.2609 Loss_G: 4.6639\n",
      "[0/25][91/782] Loss_D: 0.4297 Loss_G: 6.2335\n",
      "[0/25][92/782] Loss_D: 0.3321 Loss_G: 5.1348\n",
      "[0/25][93/782] Loss_D: 0.2122 Loss_G: 5.4415\n",
      "[0/25][94/782] Loss_D: 0.1413 Loss_G: 5.4031\n",
      "[0/25][95/782] Loss_D: 0.3529 Loss_G: 4.0167\n",
      "[0/25][96/782] Loss_D: 0.6081 Loss_G: 8.1556\n",
      "[0/25][97/782] Loss_D: 0.5949 Loss_G: 5.3862\n",
      "[0/25][98/782] Loss_D: 0.2708 Loss_G: 3.6947\n",
      "[0/25][99/782] Loss_D: 0.5257 Loss_G: 8.8306\n",
      "[0/25][100/782] Loss_D: 0.5143 Loss_G: 5.6326\n",
      "[0/25][101/782] Loss_D: 0.2109 Loss_G: 4.0311\n",
      "[0/25][102/782] Loss_D: 0.6849 Loss_G: 7.1999\n",
      "[0/25][103/782] Loss_D: 0.7827 Loss_G: 2.9115\n",
      "[0/25][104/782] Loss_D: 0.8289 Loss_G: 11.1827\n",
      "[0/25][105/782] Loss_D: 2.3032 Loss_G: 4.7188\n",
      "[0/25][106/782] Loss_D: 0.3075 Loss_G: 3.0953\n",
      "[0/25][107/782] Loss_D: 0.5519 Loss_G: 6.9988\n",
      "[0/25][108/782] Loss_D: 0.4127 Loss_G: 5.3446\n",
      "[0/25][109/782] Loss_D: 0.3234 Loss_G: 3.0938\n",
      "[0/25][110/782] Loss_D: 0.7219 Loss_G: 6.0099\n",
      "[0/25][111/782] Loss_D: 0.3217 Loss_G: 4.8414\n",
      "[0/25][112/782] Loss_D: 0.3695 Loss_G: 3.0814\n",
      "[0/25][113/782] Loss_D: 0.4701 Loss_G: 5.1607\n",
      "[0/25][114/782] Loss_D: 0.4925 Loss_G: 3.6171\n",
      "[0/25][115/782] Loss_D: 0.4867 Loss_G: 5.6080\n",
      "[0/25][116/782] Loss_D: 0.5077 Loss_G: 3.2678\n",
      "[0/25][117/782] Loss_D: 0.5390 Loss_G: 6.7327\n",
      "[0/25][118/782] Loss_D: 0.5634 Loss_G: 4.6607\n",
      "[0/25][119/782] Loss_D: 0.3921 Loss_G: 5.7622\n",
      "[0/25][120/782] Loss_D: 0.4310 Loss_G: 4.2326\n",
      "[0/25][121/782] Loss_D: 0.5307 Loss_G: 5.1555\n",
      "[0/25][122/782] Loss_D: 0.2633 Loss_G: 4.9164\n",
      "[0/25][123/782] Loss_D: 0.4950 Loss_G: 3.0232\n",
      "[0/25][124/782] Loss_D: 0.7443 Loss_G: 9.0622\n",
      "[0/25][125/782] Loss_D: 1.6276 Loss_G: 3.4156\n",
      "[0/25][126/782] Loss_D: 0.7457 Loss_G: 5.7535\n",
      "[0/25][127/782] Loss_D: 0.6780 Loss_G: 4.8214\n",
      "[0/25][128/782] Loss_D: 0.2955 Loss_G: 4.1933\n",
      "[0/25][129/782] Loss_D: 0.4497 Loss_G: 5.2065\n",
      "[0/25][130/782] Loss_D: 0.3511 Loss_G: 3.9728\n",
      "[0/25][131/782] Loss_D: 0.3012 Loss_G: 4.3593\n",
      "[0/25][132/782] Loss_D: 0.2455 Loss_G: 4.7724\n",
      "[0/25][133/782] Loss_D: 0.5826 Loss_G: 2.8656\n",
      "[0/25][134/782] Loss_D: 0.7860 Loss_G: 9.6553\n",
      "[0/25][135/782] Loss_D: 1.7487 Loss_G: 4.0304\n",
      "[0/25][136/782] Loss_D: 0.4115 Loss_G: 3.7783\n",
      "[0/25][137/782] Loss_D: 0.3599 Loss_G: 5.8271\n",
      "[0/25][138/782] Loss_D: 0.3903 Loss_G: 4.0227\n",
      "[0/25][139/782] Loss_D: 0.4956 Loss_G: 5.4906\n",
      "[0/25][140/782] Loss_D: 0.5272 Loss_G: 3.0064\n",
      "[0/25][141/782] Loss_D: 0.7000 Loss_G: 8.2087\n",
      "[0/25][142/782] Loss_D: 1.5558 Loss_G: 2.2183\n",
      "[0/25][143/782] Loss_D: 0.9556 Loss_G: 8.4143\n",
      "[0/25][144/782] Loss_D: 0.6940 Loss_G: 5.1950\n",
      "[0/25][145/782] Loss_D: 0.2380 Loss_G: 3.5263\n",
      "[0/25][146/782] Loss_D: 0.6348 Loss_G: 6.4648\n",
      "[0/25][147/782] Loss_D: 0.6471 Loss_G: 3.3452\n",
      "[0/25][148/782] Loss_D: 0.5233 Loss_G: 5.3323\n",
      "[0/25][149/782] Loss_D: 0.3164 Loss_G: 5.3415\n",
      "[0/25][150/782] Loss_D: 0.4949 Loss_G: 3.7265\n",
      "[0/25][151/782] Loss_D: 0.5493 Loss_G: 5.9930\n",
      "[0/25][152/782] Loss_D: 0.5269 Loss_G: 3.4931\n",
      "[0/25][153/782] Loss_D: 0.6450 Loss_G: 6.7237\n",
      "[0/25][154/782] Loss_D: 0.8336 Loss_G: 2.2892\n",
      "[0/25][155/782] Loss_D: 1.2053 Loss_G: 11.4301\n",
      "[0/25][156/782] Loss_D: 1.7755 Loss_G: 6.4575\n",
      "[0/25][157/782] Loss_D: 0.1444 Loss_G: 3.4869\n",
      "[0/25][158/782] Loss_D: 0.7242 Loss_G: 7.0547\n",
      "[0/25][159/782] Loss_D: 0.6702 Loss_G: 3.9176\n",
      "[0/25][160/782] Loss_D: 0.3826 Loss_G: 5.0618\n",
      "[0/25][161/782] Loss_D: 0.2566 Loss_G: 5.4873\n",
      "[0/25][162/782] Loss_D: 0.1739 Loss_G: 4.4760\n",
      "[0/25][163/782] Loss_D: 0.2981 Loss_G: 4.9094\n",
      "[0/25][164/782] Loss_D: 0.3490 Loss_G: 4.4337\n",
      "[0/25][165/782] Loss_D: 0.3781 Loss_G: 4.6442\n",
      "[0/25][166/782] Loss_D: 0.3184 Loss_G: 5.6920\n",
      "[0/25][167/782] Loss_D: 0.5183 Loss_G: 3.3971\n",
      "[0/25][168/782] Loss_D: 0.7458 Loss_G: 9.6626\n",
      "[0/25][169/782] Loss_D: 0.9012 Loss_G: 6.1141\n",
      "[0/25][170/782] Loss_D: 0.1475 Loss_G: 3.5956\n",
      "[0/25][171/782] Loss_D: 0.9047 Loss_G: 12.2095\n",
      "[0/25][172/782] Loss_D: 1.6763 Loss_G: 7.8325\n",
      "[0/25][173/782] Loss_D: 0.0856 Loss_G: 4.4545\n",
      "[0/25][174/782] Loss_D: 0.6835 Loss_G: 8.9098\n",
      "[0/25][175/782] Loss_D: 0.6814 Loss_G: 5.7430\n",
      "[0/25][176/782] Loss_D: 0.1936 Loss_G: 3.4228\n",
      "[0/25][177/782] Loss_D: 0.6693 Loss_G: 8.8894\n",
      "[0/25][178/782] Loss_D: 0.7240 Loss_G: 5.5249\n",
      "[0/25][179/782] Loss_D: 0.1653 Loss_G: 3.8425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][180/782] Loss_D: 0.4171 Loss_G: 7.4539\n",
      "[0/25][181/782] Loss_D: 0.2623 Loss_G: 6.0096\n",
      "[0/25][182/782] Loss_D: 0.1637 Loss_G: 4.9508\n",
      "[0/25][183/782] Loss_D: 0.4082 Loss_G: 7.1397\n",
      "[0/25][184/782] Loss_D: 0.3091 Loss_G: 4.5394\n",
      "[0/25][185/782] Loss_D: 0.3658 Loss_G: 7.3058\n",
      "[0/25][186/782] Loss_D: 0.2665 Loss_G: 5.5245\n",
      "[0/25][187/782] Loss_D: 0.2264 Loss_G: 5.7400\n",
      "[0/25][188/782] Loss_D: 0.1981 Loss_G: 5.0859\n",
      "[0/25][189/782] Loss_D: 0.3032 Loss_G: 6.4712\n",
      "[0/25][190/782] Loss_D: 0.1920 Loss_G: 5.5285\n",
      "[0/25][191/782] Loss_D: 0.1563 Loss_G: 5.3642\n",
      "[0/25][192/782] Loss_D: 0.1995 Loss_G: 6.0784\n",
      "[0/25][193/782] Loss_D: 0.2325 Loss_G: 4.8749\n",
      "[0/25][194/782] Loss_D: 0.2217 Loss_G: 7.1798\n",
      "[0/25][195/782] Loss_D: 0.1203 Loss_G: 6.3674\n",
      "[0/25][196/782] Loss_D: 0.1417 Loss_G: 5.1199\n",
      "[0/25][197/782] Loss_D: 0.2567 Loss_G: 7.6052\n",
      "[0/25][198/782] Loss_D: 0.2706 Loss_G: 5.2882\n",
      "[0/25][199/782] Loss_D: 0.1676 Loss_G: 7.0499\n",
      "[0/25][200/782] Loss_D: 0.1336 Loss_G: 6.3664\n",
      "[0/25][201/782] Loss_D: 0.1114 Loss_G: 6.9640\n",
      "[0/25][202/782] Loss_D: 0.1260 Loss_G: 6.8946\n",
      "[0/25][203/782] Loss_D: 0.1472 Loss_G: 6.8252\n",
      "[0/25][204/782] Loss_D: 0.1745 Loss_G: 7.0863\n",
      "[0/25][205/782] Loss_D: 0.1623 Loss_G: 7.0822\n",
      "[0/25][206/782] Loss_D: 0.1964 Loss_G: 8.2102\n",
      "[0/25][207/782] Loss_D: 0.1497 Loss_G: 6.8259\n",
      "[0/25][208/782] Loss_D: 0.1377 Loss_G: 7.5151\n",
      "[0/25][209/782] Loss_D: 0.1424 Loss_G: 7.3725\n",
      "[0/25][210/782] Loss_D: 0.2813 Loss_G: 5.9644\n",
      "[0/25][211/782] Loss_D: 0.3193 Loss_G: 13.6647\n",
      "[0/25][212/782] Loss_D: 0.0871 Loss_G: 13.4473\n",
      "[0/25][213/782] Loss_D: 0.0694 Loss_G: 10.0012\n",
      "[0/25][214/782] Loss_D: 0.1715 Loss_G: 6.1734\n",
      "[0/25][215/782] Loss_D: 0.8368 Loss_G: 20.3458\n",
      "[0/25][216/782] Loss_D: 2.7224 Loss_G: 16.0218\n",
      "[0/25][217/782] Loss_D: 0.2867 Loss_G: 10.8792\n",
      "[0/25][218/782] Loss_D: 0.1800 Loss_G: 5.1769\n",
      "[0/25][219/782] Loss_D: 2.0322 Loss_G: 19.2487\n",
      "[0/25][220/782] Loss_D: 3.7945 Loss_G: 16.5413\n",
      "[0/25][221/782] Loss_D: 0.0480 Loss_G: 12.9894\n",
      "[0/25][222/782] Loss_D: 0.0148 Loss_G: 7.7794\n",
      "[0/25][223/782] Loss_D: 0.1438 Loss_G: 5.4769\n",
      "[0/25][224/782] Loss_D: 0.3709 Loss_G: 8.1926\n",
      "[0/25][225/782] Loss_D: 0.1375 Loss_G: 7.0128\n",
      "[0/25][226/782] Loss_D: 0.4630 Loss_G: 5.8254\n",
      "[0/25][227/782] Loss_D: 0.7710 Loss_G: 10.7061\n",
      "[0/25][228/782] Loss_D: 1.5589 Loss_G: 4.9573\n",
      "[0/25][229/782] Loss_D: 0.4425 Loss_G: 7.1382\n",
      "[0/25][230/782] Loss_D: 0.1744 Loss_G: 7.1983\n",
      "[0/25][231/782] Loss_D: 0.1168 Loss_G: 6.8194\n",
      "[0/25][232/782] Loss_D: 0.2680 Loss_G: 7.0292\n",
      "[0/25][233/782] Loss_D: 0.2481 Loss_G: 6.6979\n",
      "[0/25][234/782] Loss_D: 0.1925 Loss_G: 6.1523\n",
      "[0/25][235/782] Loss_D: 0.1266 Loss_G: 6.4869\n",
      "[0/25][236/782] Loss_D: 0.1695 Loss_G: 6.3579\n",
      "[0/25][237/782] Loss_D: 0.1689 Loss_G: 5.2714\n",
      "[0/25][238/782] Loss_D: 0.3668 Loss_G: 10.1802\n",
      "[0/25][239/782] Loss_D: 0.3014 Loss_G: 7.8872\n",
      "[0/25][240/782] Loss_D: 0.1289 Loss_G: 4.8927\n",
      "[0/25][241/782] Loss_D: 0.9244 Loss_G: 12.3713\n",
      "[0/25][242/782] Loss_D: 0.5859 Loss_G: 10.2086\n",
      "[0/25][243/782] Loss_D: 0.0787 Loss_G: 6.1513\n",
      "[0/25][244/782] Loss_D: 0.2600 Loss_G: 7.9398\n",
      "[0/25][245/782] Loss_D: 0.2938 Loss_G: 6.5332\n",
      "[0/25][246/782] Loss_D: 0.1673 Loss_G: 5.1113\n",
      "[0/25][247/782] Loss_D: 0.3986 Loss_G: 9.2916\n",
      "[0/25][248/782] Loss_D: 0.7351 Loss_G: 3.0008\n",
      "[0/25][249/782] Loss_D: 0.9501 Loss_G: 12.7682\n",
      "[0/25][250/782] Loss_D: 1.4490 Loss_G: 7.7213\n",
      "[0/25][251/782] Loss_D: 0.0423 Loss_G: 3.8217\n",
      "[0/25][252/782] Loss_D: 0.7432 Loss_G: 9.1815\n",
      "[0/25][253/782] Loss_D: 0.0565 Loss_G: 9.5924\n",
      "[0/25][254/782] Loss_D: 0.2439 Loss_G: 7.1892\n",
      "[0/25][255/782] Loss_D: 0.2991 Loss_G: 3.7050\n",
      "[0/25][256/782] Loss_D: 0.2908 Loss_G: 5.6769\n",
      "[0/25][257/782] Loss_D: 0.2206 Loss_G: 6.6521\n",
      "[0/25][258/782] Loss_D: 0.2235 Loss_G: 5.3602\n",
      "[0/25][259/782] Loss_D: 0.6124 Loss_G: 2.7185\n",
      "[0/25][260/782] Loss_D: 0.7932 Loss_G: 8.9725\n",
      "[0/25][261/782] Loss_D: 0.8431 Loss_G: 5.5560\n",
      "[0/25][262/782] Loss_D: 0.1782 Loss_G: 3.7805\n",
      "[0/25][263/782] Loss_D: 0.3624 Loss_G: 6.8274\n",
      "[0/25][264/782] Loss_D: 0.1430 Loss_G: 6.5496\n",
      "[0/25][265/782] Loss_D: 0.2896 Loss_G: 3.9816\n",
      "[0/25][266/782] Loss_D: 0.2136 Loss_G: 4.6339\n",
      "[0/25][267/782] Loss_D: 0.1600 Loss_G: 5.4639\n",
      "[0/25][268/782] Loss_D: 0.1546 Loss_G: 5.5493\n",
      "[0/25][269/782] Loss_D: 0.1146 Loss_G: 5.0305\n",
      "[0/25][270/782] Loss_D: 0.1904 Loss_G: 5.0340\n",
      "[0/25][271/782] Loss_D: 0.1413 Loss_G: 5.3694\n",
      "[0/25][272/782] Loss_D: 0.1874 Loss_G: 4.3514\n",
      "[0/25][273/782] Loss_D: 0.1438 Loss_G: 5.1130\n",
      "[0/25][274/782] Loss_D: 0.1022 Loss_G: 5.6089\n",
      "[0/25][275/782] Loss_D: 0.1147 Loss_G: 5.4298\n",
      "[0/25][276/782] Loss_D: 0.0970 Loss_G: 5.6360\n",
      "[0/25][277/782] Loss_D: 0.1006 Loss_G: 5.4736\n",
      "[0/25][278/782] Loss_D: 0.0745 Loss_G: 5.7368\n",
      "[0/25][279/782] Loss_D: 0.1258 Loss_G: 6.4629\n",
      "[0/25][280/782] Loss_D: 0.1604 Loss_G: 5.2989\n",
      "[0/25][281/782] Loss_D: 0.1462 Loss_G: 6.2517\n",
      "[0/25][282/782] Loss_D: 0.1508 Loss_G: 6.0801\n",
      "[0/25][283/782] Loss_D: 0.1579 Loss_G: 6.1763\n",
      "[0/25][284/782] Loss_D: 0.3372 Loss_G: 6.6162\n",
      "[0/25][285/782] Loss_D: 0.1192 Loss_G: 7.2876\n",
      "[0/25][286/782] Loss_D: 0.1922 Loss_G: 5.3214\n",
      "[0/25][287/782] Loss_D: 0.5691 Loss_G: 13.7914\n",
      "[0/25][288/782] Loss_D: 1.3791 Loss_G: 7.5839\n",
      "[0/25][289/782] Loss_D: 0.3512 Loss_G: 6.2765\n",
      "[0/25][290/782] Loss_D: 0.3509 Loss_G: 9.0658\n",
      "[0/25][291/782] Loss_D: 0.3015 Loss_G: 6.7150\n",
      "[0/25][292/782] Loss_D: 0.1287 Loss_G: 5.5810\n",
      "[0/25][293/782] Loss_D: 0.3100 Loss_G: 8.8928\n",
      "[0/25][294/782] Loss_D: 0.1927 Loss_G: 7.5548\n",
      "[0/25][295/782] Loss_D: 0.2268 Loss_G: 4.3837\n",
      "[0/25][296/782] Loss_D: 0.5018 Loss_G: 11.3661\n",
      "[0/25][297/782] Loss_D: 0.6175 Loss_G: 8.2269\n",
      "[0/25][298/782] Loss_D: 0.0633 Loss_G: 5.3027\n",
      "[0/25][299/782] Loss_D: 0.2739 Loss_G: 6.9810\n",
      "[0/25][300/782] Loss_D: 0.1685 Loss_G: 7.3822\n",
      "[0/25][301/782] Loss_D: 0.2191 Loss_G: 5.3579\n",
      "[0/25][302/782] Loss_D: 0.4939 Loss_G: 9.7101\n",
      "[0/25][303/782] Loss_D: 0.5496 Loss_G: 7.7680\n",
      "[0/25][304/782] Loss_D: 0.1299 Loss_G: 5.3161\n",
      "[0/25][305/782] Loss_D: 0.3963 Loss_G: 10.0143\n",
      "[0/25][306/782] Loss_D: 0.1970 Loss_G: 9.1886\n",
      "[0/25][307/782] Loss_D: 0.5240 Loss_G: 4.3841\n",
      "[0/25][308/782] Loss_D: 0.3938 Loss_G: 8.8013\n",
      "[0/25][309/782] Loss_D: 0.1456 Loss_G: 8.2970\n",
      "[0/25][310/782] Loss_D: 0.1246 Loss_G: 6.2087\n",
      "[0/25][311/782] Loss_D: 0.1143 Loss_G: 5.8354\n",
      "[0/25][312/782] Loss_D: 0.0992 Loss_G: 6.9845\n",
      "[0/25][313/782] Loss_D: 0.2317 Loss_G: 5.5839\n",
      "[0/25][314/782] Loss_D: 0.1639 Loss_G: 6.4004\n",
      "[0/25][315/782] Loss_D: 0.2187 Loss_G: 5.8574\n",
      "[0/25][316/782] Loss_D: 0.1655 Loss_G: 5.8223\n",
      "[0/25][317/782] Loss_D: 0.1623 Loss_G: 6.8733\n",
      "[0/25][318/782] Loss_D: 0.2258 Loss_G: 5.7030\n",
      "[0/25][319/782] Loss_D: 0.3239 Loss_G: 10.0321\n",
      "[0/25][320/782] Loss_D: 0.3829 Loss_G: 8.2994\n",
      "[0/25][321/782] Loss_D: 0.0571 Loss_G: 6.6864\n",
      "[0/25][322/782] Loss_D: 0.1960 Loss_G: 8.7988\n",
      "[0/25][323/782] Loss_D: 0.0365 Loss_G: 8.2802\n",
      "[0/25][324/782] Loss_D: 0.1149 Loss_G: 5.8577\n",
      "[0/25][325/782] Loss_D: 0.1284 Loss_G: 5.2439\n",
      "[0/25][326/782] Loss_D: 0.1250 Loss_G: 7.5399\n",
      "[0/25][327/782] Loss_D: 0.0839 Loss_G: 7.2515\n",
      "[0/25][328/782] Loss_D: 0.0479 Loss_G: 5.9884\n",
      "[0/25][329/782] Loss_D: 0.0591 Loss_G: 5.3955\n",
      "[0/25][330/782] Loss_D: 0.1099 Loss_G: 6.2128\n",
      "[0/25][331/782] Loss_D: 0.0623 Loss_G: 6.2778\n",
      "[0/25][332/782] Loss_D: 0.0812 Loss_G: 6.6959\n",
      "[0/25][333/782] Loss_D: 0.1445 Loss_G: 6.1597\n",
      "[0/25][334/782] Loss_D: 0.1503 Loss_G: 7.0040\n",
      "[0/25][335/782] Loss_D: 0.1582 Loss_G: 5.8232\n",
      "[0/25][336/782] Loss_D: 0.1271 Loss_G: 7.1656\n",
      "[0/25][337/782] Loss_D: 0.1012 Loss_G: 6.4593\n",
      "[0/25][338/782] Loss_D: 0.0634 Loss_G: 6.0670\n",
      "[0/25][339/782] Loss_D: 0.1251 Loss_G: 7.8507\n",
      "[0/25][340/782] Loss_D: 0.0919 Loss_G: 6.8471\n",
      "[0/25][341/782] Loss_D: 0.1091 Loss_G: 8.7488\n",
      "[0/25][342/782] Loss_D: 0.2181 Loss_G: 6.7971\n",
      "[0/25][343/782] Loss_D: 0.4500 Loss_G: 17.2366\n",
      "[0/25][344/782] Loss_D: 1.2869 Loss_G: 12.3203\n",
      "[0/25][345/782] Loss_D: 0.0542 Loss_G: 5.8902\n",
      "[0/25][346/782] Loss_D: 0.6992 Loss_G: 19.1328\n",
      "[0/25][347/782] Loss_D: 0.3366 Loss_G: 19.1836\n",
      "[0/25][348/782] Loss_D: 0.1302 Loss_G: 16.2124\n",
      "[0/25][349/782] Loss_D: 0.0289 Loss_G: 11.0866\n",
      "[0/25][350/782] Loss_D: 0.0953 Loss_G: 6.3947\n",
      "[0/25][351/782] Loss_D: 1.6995 Loss_G: 21.7429\n",
      "[0/25][352/782] Loss_D: 4.7106 Loss_G: 17.7200\n",
      "[0/25][353/782] Loss_D: 0.7808 Loss_G: 11.4242\n",
      "[0/25][354/782] Loss_D: 0.2432 Loss_G: 8.4613\n",
      "[0/25][355/782] Loss_D: 0.7079 Loss_G: 11.8744\n",
      "[0/25][356/782] Loss_D: 0.1297 Loss_G: 10.6185\n",
      "[0/25][357/782] Loss_D: 0.3135 Loss_G: 5.4920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][358/782] Loss_D: 2.0887 Loss_G: 16.3032\n",
      "[0/25][359/782] Loss_D: 3.9616 Loss_G: 11.6658\n",
      "[0/25][360/782] Loss_D: 0.3198 Loss_G: 8.2352\n",
      "[0/25][361/782] Loss_D: 0.3710 Loss_G: 6.7083\n",
      "[0/25][362/782] Loss_D: 0.3439 Loss_G: 7.8645\n",
      "[0/25][363/782] Loss_D: 0.2225 Loss_G: 6.3732\n",
      "[0/25][364/782] Loss_D: 0.3701 Loss_G: 6.4199\n",
      "[0/25][365/782] Loss_D: 0.3049 Loss_G: 6.4325\n",
      "[0/25][366/782] Loss_D: 0.2456 Loss_G: 5.9837\n",
      "[0/25][367/782] Loss_D: 0.3825 Loss_G: 6.5897\n",
      "[0/25][368/782] Loss_D: 0.4946 Loss_G: 5.5142\n",
      "[0/25][369/782] Loss_D: 0.4618 Loss_G: 11.5774\n",
      "[0/25][370/782] Loss_D: 0.8604 Loss_G: 8.2482\n",
      "[0/25][371/782] Loss_D: 0.1391 Loss_G: 4.2777\n",
      "[0/25][372/782] Loss_D: 1.3361 Loss_G: 14.0497\n",
      "[0/25][373/782] Loss_D: 1.7566 Loss_G: 11.0022\n",
      "[0/25][374/782] Loss_D: 0.3865 Loss_G: 6.3632\n",
      "[0/25][375/782] Loss_D: 0.2946 Loss_G: 5.7419\n",
      "[0/25][376/782] Loss_D: 0.3822 Loss_G: 7.7864\n",
      "[0/25][377/782] Loss_D: 0.2182 Loss_G: 6.7566\n",
      "[0/25][378/782] Loss_D: 0.2447 Loss_G: 5.2041\n",
      "[0/25][379/782] Loss_D: 0.5587 Loss_G: 7.3573\n",
      "[0/25][380/782] Loss_D: 0.5301 Loss_G: 4.1902\n",
      "[0/25][381/782] Loss_D: 0.6824 Loss_G: 12.3390\n",
      "[0/25][382/782] Loss_D: 1.2283 Loss_G: 7.5023\n",
      "[0/25][383/782] Loss_D: 0.2017 Loss_G: 5.0406\n",
      "[0/25][384/782] Loss_D: 0.4528 Loss_G: 10.1282\n",
      "[0/25][385/782] Loss_D: 0.2191 Loss_G: 7.3113\n",
      "[0/25][386/782] Loss_D: 0.5985 Loss_G: 8.3771\n",
      "[0/25][387/782] Loss_D: 0.4635 Loss_G: 4.7259\n",
      "[0/25][388/782] Loss_D: 0.9124 Loss_G: 9.6658\n",
      "[0/25][389/782] Loss_D: 0.8149 Loss_G: 7.3975\n",
      "[0/25][390/782] Loss_D: 0.1097 Loss_G: 4.8356\n",
      "[0/25][391/782] Loss_D: 0.1674 Loss_G: 5.1841\n",
      "[0/25][392/782] Loss_D: 0.1185 Loss_G: 5.8761\n",
      "[0/25][393/782] Loss_D: 0.0492 Loss_G: 5.8840\n",
      "[0/25][394/782] Loss_D: 0.0717 Loss_G: 5.3525\n",
      "[0/25][395/782] Loss_D: 0.1127 Loss_G: 5.2278\n",
      "[0/25][396/782] Loss_D: 0.2260 Loss_G: 4.9951\n",
      "[0/25][397/782] Loss_D: 0.1777 Loss_G: 5.2743\n",
      "[0/25][398/782] Loss_D: 0.1548 Loss_G: 5.2429\n",
      "[0/25][399/782] Loss_D: 0.1169 Loss_G: 5.5726\n",
      "[0/25][400/782] Loss_D: 0.1659 Loss_G: 5.1502\n",
      "[0/25][401/782] Loss_D: 0.1413 Loss_G: 5.5615\n",
      "[0/25][402/782] Loss_D: 0.1475 Loss_G: 5.3287\n",
      "[0/25][403/782] Loss_D: 0.1885 Loss_G: 5.6633\n",
      "[0/25][404/782] Loss_D: 0.1948 Loss_G: 5.0572\n",
      "[0/25][405/782] Loss_D: 0.1677 Loss_G: 5.2395\n",
      "[0/25][406/782] Loss_D: 0.1441 Loss_G: 6.0296\n",
      "[0/25][407/782] Loss_D: 0.1783 Loss_G: 4.9641\n",
      "[0/25][408/782] Loss_D: 0.3906 Loss_G: 5.1318\n",
      "[0/25][409/782] Loss_D: 0.1432 Loss_G: 7.3521\n",
      "[0/25][410/782] Loss_D: 0.1011 Loss_G: 6.2233\n",
      "[0/25][411/782] Loss_D: 0.1485 Loss_G: 4.7555\n",
      "[0/25][412/782] Loss_D: 0.2960 Loss_G: 7.0117\n",
      "[0/25][413/782] Loss_D: 0.1919 Loss_G: 6.4941\n",
      "[0/25][414/782] Loss_D: 0.2106 Loss_G: 4.8259\n",
      "[0/25][415/782] Loss_D: 0.3066 Loss_G: 9.6939\n",
      "[0/25][416/782] Loss_D: 0.1369 Loss_G: 9.1525\n",
      "[0/25][417/782] Loss_D: 0.0541 Loss_G: 7.1608\n",
      "[0/25][418/782] Loss_D: 0.0816 Loss_G: 4.9661\n",
      "[0/25][419/782] Loss_D: 0.4375 Loss_G: 10.4744\n",
      "[0/25][420/782] Loss_D: 0.2516 Loss_G: 9.7542\n",
      "[0/25][421/782] Loss_D: 0.2555 Loss_G: 5.9172\n",
      "[0/25][422/782] Loss_D: 0.6804 Loss_G: 9.9731\n",
      "[0/25][423/782] Loss_D: 0.5424 Loss_G: 9.7695\n",
      "[0/25][424/782] Loss_D: 0.3359 Loss_G: 6.1466\n",
      "[0/25][425/782] Loss_D: 0.2379 Loss_G: 6.0776\n",
      "[0/25][426/782] Loss_D: 0.1647 Loss_G: 8.6084\n",
      "[0/25][427/782] Loss_D: 0.1182 Loss_G: 7.4676\n",
      "[0/25][428/782] Loss_D: 0.0613 Loss_G: 6.0838\n",
      "[0/25][429/782] Loss_D: 0.1370 Loss_G: 7.8316\n",
      "[0/25][430/782] Loss_D: 0.4182 Loss_G: 3.7133\n",
      "[0/25][431/782] Loss_D: 0.6822 Loss_G: 13.0814\n",
      "[0/25][432/782] Loss_D: 0.9614 Loss_G: 10.2537\n",
      "[0/25][433/782] Loss_D: 0.1294 Loss_G: 6.4997\n",
      "[0/25][434/782] Loss_D: 0.2415 Loss_G: 6.4065\n",
      "[0/25][435/782] Loss_D: 0.2032 Loss_G: 7.0884\n",
      "[0/25][436/782] Loss_D: 0.1530 Loss_G: 5.4760\n",
      "[0/25][437/782] Loss_D: 0.1422 Loss_G: 6.4195\n",
      "[0/25][438/782] Loss_D: 0.0920 Loss_G: 6.1693\n",
      "[0/25][439/782] Loss_D: 0.2031 Loss_G: 5.5177\n",
      "[0/25][440/782] Loss_D: 0.4061 Loss_G: 5.2193\n",
      "[0/25][441/782] Loss_D: 0.4164 Loss_G: 8.0681\n",
      "[0/25][442/782] Loss_D: 0.5177 Loss_G: 4.4903\n",
      "[0/25][443/782] Loss_D: 0.3111 Loss_G: 7.4728\n",
      "[0/25][444/782] Loss_D: 0.0906 Loss_G: 7.1705\n",
      "[0/25][445/782] Loss_D: 0.1050 Loss_G: 5.6922\n",
      "[0/25][446/782] Loss_D: 0.1563 Loss_G: 6.1283\n",
      "[0/25][447/782] Loss_D: 0.1574 Loss_G: 5.8628\n",
      "[0/25][448/782] Loss_D: 0.1191 Loss_G: 6.1650\n",
      "[0/25][449/782] Loss_D: 0.4888 Loss_G: 2.5523\n",
      "[0/25][450/782] Loss_D: 0.7163 Loss_G: 11.2461\n",
      "[0/25][451/782] Loss_D: 0.4248 Loss_G: 10.4607\n",
      "[0/25][452/782] Loss_D: 0.1654 Loss_G: 7.9559\n",
      "[0/25][453/782] Loss_D: 0.0881 Loss_G: 5.9431\n",
      "[0/25][454/782] Loss_D: 0.1453 Loss_G: 6.4674\n",
      "[0/25][455/782] Loss_D: 0.1923 Loss_G: 9.1004\n",
      "[0/25][456/782] Loss_D: 0.3116 Loss_G: 5.9489\n",
      "[0/25][457/782] Loss_D: 0.0682 Loss_G: 4.3382\n",
      "[0/25][458/782] Loss_D: 0.2282 Loss_G: 7.7186\n",
      "[0/25][459/782] Loss_D: 0.1447 Loss_G: 6.8706\n",
      "[0/25][460/782] Loss_D: 0.0771 Loss_G: 5.3221\n",
      "[0/25][461/782] Loss_D: 0.0914 Loss_G: 4.9915\n",
      "[0/25][462/782] Loss_D: 0.0572 Loss_G: 5.0784\n",
      "[0/25][463/782] Loss_D: 0.0811 Loss_G: 4.8296\n",
      "[0/25][464/782] Loss_D: 0.1355 Loss_G: 5.2674\n",
      "[0/25][465/782] Loss_D: 0.1284 Loss_G: 5.0207\n",
      "[0/25][466/782] Loss_D: 0.1639 Loss_G: 5.0188\n",
      "[0/25][467/782] Loss_D: 0.0956 Loss_G: 5.6724\n",
      "[0/25][468/782] Loss_D: 0.0835 Loss_G: 5.6283\n",
      "[0/25][469/782] Loss_D: 0.3175 Loss_G: 4.4363\n",
      "[0/25][470/782] Loss_D: 0.3214 Loss_G: 8.5419\n",
      "[0/25][471/782] Loss_D: 0.3078 Loss_G: 7.3666\n",
      "[0/25][472/782] Loss_D: 0.0645 Loss_G: 5.7671\n",
      "[0/25][473/782] Loss_D: 0.0356 Loss_G: 4.9264\n",
      "[0/25][474/782] Loss_D: 0.1059 Loss_G: 5.8120\n",
      "[0/25][475/782] Loss_D: 0.0579 Loss_G: 6.1635\n",
      "[0/25][476/782] Loss_D: 0.0428 Loss_G: 5.8263\n",
      "[0/25][477/782] Loss_D: 0.0546 Loss_G: 5.2062\n",
      "[0/25][478/782] Loss_D: 0.2565 Loss_G: 5.1373\n",
      "[0/25][479/782] Loss_D: 0.1019 Loss_G: 5.8977\n",
      "[0/25][480/782] Loss_D: 0.0991 Loss_G: 5.3389\n",
      "[0/25][481/782] Loss_D: 0.1155 Loss_G: 5.5827\n",
      "[0/25][482/782] Loss_D: 0.1617 Loss_G: 4.2628\n",
      "[0/25][483/782] Loss_D: 0.2495 Loss_G: 6.3376\n",
      "[0/25][484/782] Loss_D: 0.1567 Loss_G: 5.2275\n",
      "[0/25][485/782] Loss_D: 0.1102 Loss_G: 5.5151\n",
      "[0/25][486/782] Loss_D: 0.1560 Loss_G: 5.8297\n",
      "[0/25][487/782] Loss_D: 0.1496 Loss_G: 6.1479\n",
      "[0/25][488/782] Loss_D: 0.0563 Loss_G: 5.8946\n",
      "[0/25][489/782] Loss_D: 0.1922 Loss_G: 6.5595\n",
      "[0/25][490/782] Loss_D: 0.2414 Loss_G: 5.0936\n",
      "[0/25][491/782] Loss_D: 0.5226 Loss_G: 13.4572\n",
      "[0/25][492/782] Loss_D: 1.9119 Loss_G: 6.6523\n",
      "[0/25][493/782] Loss_D: 0.2998 Loss_G: 6.1360\n",
      "[0/25][494/782] Loss_D: 0.3514 Loss_G: 8.1105\n",
      "[0/25][495/782] Loss_D: 0.4154 Loss_G: 3.8084\n",
      "[0/25][496/782] Loss_D: 1.2160 Loss_G: 13.0140\n",
      "[0/25][497/782] Loss_D: 2.1531 Loss_G: 6.2692\n",
      "[0/25][498/782] Loss_D: 0.2889 Loss_G: 4.9132\n",
      "[0/25][499/782] Loss_D: 0.4502 Loss_G: 7.1386\n",
      "[0/25][500/782] Loss_D: 0.2782 Loss_G: 6.2009\n",
      "[0/25][501/782] Loss_D: 0.4211 Loss_G: 5.3598\n",
      "[0/25][502/782] Loss_D: 0.4128 Loss_G: 5.6993\n",
      "[0/25][503/782] Loss_D: 0.2189 Loss_G: 6.6847\n",
      "[0/25][504/782] Loss_D: 0.1927 Loss_G: 5.7366\n",
      "[0/25][505/782] Loss_D: 0.2013 Loss_G: 6.2227\n",
      "[0/25][506/782] Loss_D: 0.2635 Loss_G: 5.0261\n",
      "[0/25][507/782] Loss_D: 0.1762 Loss_G: 6.7705\n",
      "[0/25][508/782] Loss_D: 0.1107 Loss_G: 6.4763\n",
      "[0/25][509/782] Loss_D: 0.1752 Loss_G: 5.6825\n",
      "[0/25][510/782] Loss_D: 0.2144 Loss_G: 7.7110\n",
      "[0/25][511/782] Loss_D: 0.2041 Loss_G: 6.6791\n",
      "[0/25][512/782] Loss_D: 0.1015 Loss_G: 6.7919\n",
      "[0/25][513/782] Loss_D: 0.1533 Loss_G: 5.4504\n",
      "[0/25][514/782] Loss_D: 0.3714 Loss_G: 13.0421\n",
      "[0/25][515/782] Loss_D: 0.4019 Loss_G: 11.9499\n",
      "[0/25][516/782] Loss_D: 0.0577 Loss_G: 7.9901\n",
      "[0/25][517/782] Loss_D: 0.0897 Loss_G: 5.9961\n",
      "[0/25][518/782] Loss_D: 0.4871 Loss_G: 12.6600\n",
      "[0/25][519/782] Loss_D: 0.6802 Loss_G: 8.9538\n",
      "[0/25][520/782] Loss_D: 0.2742 Loss_G: 5.2098\n",
      "[0/25][521/782] Loss_D: 0.7773 Loss_G: 18.1810\n",
      "[0/25][522/782] Loss_D: 3.3608 Loss_G: 8.2922\n",
      "[0/25][523/782] Loss_D: 0.0762 Loss_G: 2.6262\n",
      "[0/25][524/782] Loss_D: 0.9130 Loss_G: 14.1667\n",
      "[0/25][525/782] Loss_D: 1.9644 Loss_G: 7.8521\n",
      "[0/25][526/782] Loss_D: 0.0690 Loss_G: 4.4576\n",
      "[0/25][527/782] Loss_D: 0.7133 Loss_G: 8.0709\n",
      "[0/25][528/782] Loss_D: 0.2920 Loss_G: 6.6374\n",
      "[0/25][529/782] Loss_D: 0.6083 Loss_G: 5.8240\n",
      "[0/25][530/782] Loss_D: 0.7823 Loss_G: 4.7061\n",
      "[0/25][531/782] Loss_D: 1.2150 Loss_G: 5.5501\n",
      "[0/25][532/782] Loss_D: 0.6877 Loss_G: 4.2959\n",
      "[0/25][533/782] Loss_D: 0.8180 Loss_G: 7.0597\n",
      "[0/25][534/782] Loss_D: 0.5918 Loss_G: 4.5442\n",
      "[0/25][535/782] Loss_D: 0.6155 Loss_G: 5.0875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][536/782] Loss_D: 0.3950 Loss_G: 5.8273\n",
      "[0/25][537/782] Loss_D: 0.3454 Loss_G: 4.8852\n",
      "[0/25][538/782] Loss_D: 0.4373 Loss_G: 3.9449\n",
      "[0/25][539/782] Loss_D: 0.5492 Loss_G: 5.7318\n",
      "[0/25][540/782] Loss_D: 0.2892 Loss_G: 5.5115\n",
      "[0/25][541/782] Loss_D: 0.3512 Loss_G: 4.9280\n",
      "[0/25][542/782] Loss_D: 0.7589 Loss_G: 5.4104\n",
      "[0/25][543/782] Loss_D: 0.6978 Loss_G: 4.2285\n",
      "[0/25][544/782] Loss_D: 1.0928 Loss_G: 3.8645\n",
      "[0/25][545/782] Loss_D: 0.4940 Loss_G: 5.0116\n",
      "[0/25][546/782] Loss_D: 0.7203 Loss_G: 2.4739\n",
      "[0/25][547/782] Loss_D: 1.1858 Loss_G: 7.6023\n",
      "[0/25][548/782] Loss_D: 0.9450 Loss_G: 4.7479\n",
      "[0/25][549/782] Loss_D: 0.3938 Loss_G: 3.8455\n",
      "[0/25][550/782] Loss_D: 0.4392 Loss_G: 4.6858\n",
      "[0/25][551/782] Loss_D: 0.4471 Loss_G: 3.5081\n",
      "[0/25][552/782] Loss_D: 0.7638 Loss_G: 2.5977\n",
      "[0/25][553/782] Loss_D: 0.8578 Loss_G: 5.8452\n",
      "[0/25][554/782] Loss_D: 0.8038 Loss_G: 3.4920\n",
      "[0/25][555/782] Loss_D: 0.3065 Loss_G: 3.2765\n",
      "[0/25][556/782] Loss_D: 0.6530 Loss_G: 3.0711\n",
      "[0/25][557/782] Loss_D: 0.8273 Loss_G: 5.4057\n",
      "[0/25][558/782] Loss_D: 0.5736 Loss_G: 3.6497\n",
      "[0/25][559/782] Loss_D: 1.0474 Loss_G: 1.4513\n",
      "[0/25][560/782] Loss_D: 1.5241 Loss_G: 7.5415\n",
      "[0/25][561/782] Loss_D: 1.3136 Loss_G: 3.3728\n",
      "[0/25][562/782] Loss_D: 0.5049 Loss_G: 2.8975\n",
      "[0/25][563/782] Loss_D: 0.5716 Loss_G: 3.5836\n",
      "[0/25][564/782] Loss_D: 0.7628 Loss_G: 2.6839\n",
      "[0/25][565/782] Loss_D: 0.6209 Loss_G: 3.9250\n",
      "[0/25][566/782] Loss_D: 0.5876 Loss_G: 3.2919\n",
      "[0/25][567/782] Loss_D: 0.5140 Loss_G: 3.0945\n",
      "[0/25][568/782] Loss_D: 0.4801 Loss_G: 3.7989\n",
      "[0/25][569/782] Loss_D: 0.5599 Loss_G: 3.6190\n",
      "[0/25][570/782] Loss_D: 0.6798 Loss_G: 4.1018\n",
      "[0/25][571/782] Loss_D: 1.0094 Loss_G: 1.4951\n",
      "[0/25][572/782] Loss_D: 1.0670 Loss_G: 7.8879\n",
      "[0/25][573/782] Loss_D: 1.3372 Loss_G: 4.8357\n",
      "[0/25][574/782] Loss_D: 0.3384 Loss_G: 2.4177\n",
      "[0/25][575/782] Loss_D: 0.9636 Loss_G: 6.6272\n",
      "[0/25][576/782] Loss_D: 0.8616 Loss_G: 3.2360\n",
      "[0/25][577/782] Loss_D: 1.5001 Loss_G: 3.3989\n",
      "[0/25][578/782] Loss_D: 1.2859 Loss_G: 4.8700\n",
      "[0/25][579/782] Loss_D: 1.0641 Loss_G: 2.7445\n",
      "[0/25][580/782] Loss_D: 0.7604 Loss_G: 6.7138\n",
      "[0/25][581/782] Loss_D: 0.5261 Loss_G: 5.0046\n",
      "[0/25][582/782] Loss_D: 0.2383 Loss_G: 3.1649\n",
      "[0/25][583/782] Loss_D: 0.5295 Loss_G: 5.1975\n",
      "[0/25][584/782] Loss_D: 0.2170 Loss_G: 4.9856\n",
      "[0/25][585/782] Loss_D: 0.6977 Loss_G: 2.5988\n",
      "[0/25][586/782] Loss_D: 0.7581 Loss_G: 5.9340\n",
      "[0/25][587/782] Loss_D: 0.5847 Loss_G: 3.8646\n",
      "[0/25][588/782] Loss_D: 0.6997 Loss_G: 3.6198\n",
      "[0/25][589/782] Loss_D: 0.6433 Loss_G: 6.8524\n",
      "[0/25][590/782] Loss_D: 0.6311 Loss_G: 4.0723\n",
      "[0/25][591/782] Loss_D: 0.4426 Loss_G: 6.1489\n",
      "[0/25][592/782] Loss_D: 0.2485 Loss_G: 5.4973\n",
      "[0/25][593/782] Loss_D: 0.1934 Loss_G: 5.0934\n",
      "[0/25][594/782] Loss_D: 0.3824 Loss_G: 4.2176\n",
      "[0/25][595/782] Loss_D: 0.4754 Loss_G: 4.7362\n",
      "[0/25][596/782] Loss_D: 0.3623 Loss_G: 4.9581\n",
      "[0/25][597/782] Loss_D: 0.4531 Loss_G: 3.4847\n",
      "[0/25][598/782] Loss_D: 0.6192 Loss_G: 7.3953\n",
      "[0/25][599/782] Loss_D: 0.6494 Loss_G: 4.0028\n",
      "[0/25][600/782] Loss_D: 0.2590 Loss_G: 4.4946\n",
      "[0/25][601/782] Loss_D: 0.2947 Loss_G: 5.8157\n",
      "[0/25][602/782] Loss_D: 0.1126 Loss_G: 5.7828\n",
      "[0/25][603/782] Loss_D: 0.2282 Loss_G: 4.0389\n",
      "[0/25][604/782] Loss_D: 0.3931 Loss_G: 6.8254\n",
      "[0/25][605/782] Loss_D: 0.4624 Loss_G: 3.2298\n",
      "[0/25][606/782] Loss_D: 0.7160 Loss_G: 8.7759\n",
      "[0/25][607/782] Loss_D: 0.7135 Loss_G: 3.6805\n",
      "[0/25][608/782] Loss_D: 0.5459 Loss_G: 9.6024\n",
      "[0/25][609/782] Loss_D: 0.3804 Loss_G: 6.3130\n",
      "[0/25][610/782] Loss_D: 0.3538 Loss_G: 1.3834\n",
      "[0/25][611/782] Loss_D: 1.6914 Loss_G: 10.1432\n",
      "[0/25][612/782] Loss_D: 2.8678 Loss_G: 1.2262\n",
      "[0/25][613/782] Loss_D: 1.3478 Loss_G: 5.2159\n",
      "[0/25][614/782] Loss_D: 0.7959 Loss_G: 2.7616\n",
      "[0/25][615/782] Loss_D: 0.5836 Loss_G: 3.6438\n",
      "[0/25][616/782] Loss_D: 0.4283 Loss_G: 3.7678\n",
      "[0/25][617/782] Loss_D: 0.3645 Loss_G: 3.5263\n",
      "[0/25][618/782] Loss_D: 0.4971 Loss_G: 3.2920\n",
      "[0/25][619/782] Loss_D: 0.2792 Loss_G: 4.0190\n",
      "[0/25][620/782] Loss_D: 0.4160 Loss_G: 2.3336\n",
      "[0/25][621/782] Loss_D: 0.4142 Loss_G: 4.6274\n",
      "[0/25][622/782] Loss_D: 0.1199 Loss_G: 5.4547\n",
      "[0/25][623/782] Loss_D: 0.2528 Loss_G: 3.2895\n",
      "[0/25][624/782] Loss_D: 0.4627 Loss_G: 3.9667\n",
      "[0/25][625/782] Loss_D: 0.3784 Loss_G: 2.8590\n",
      "[0/25][626/782] Loss_D: 0.5527 Loss_G: 7.5191\n",
      "[0/25][627/782] Loss_D: 0.8861 Loss_G: 4.2776\n",
      "[0/25][628/782] Loss_D: 0.2674 Loss_G: 2.8028\n",
      "[0/25][629/782] Loss_D: 0.9177 Loss_G: 8.4882\n",
      "[0/25][630/782] Loss_D: 0.5867 Loss_G: 6.9769\n",
      "[0/25][631/782] Loss_D: 0.4162 Loss_G: 1.5452\n",
      "[0/25][632/782] Loss_D: 1.6370 Loss_G: 10.7559\n",
      "[0/25][633/782] Loss_D: 1.4404 Loss_G: 4.0698\n",
      "[0/25][634/782] Loss_D: 0.5943 Loss_G: 2.8507\n",
      "[0/25][635/782] Loss_D: 1.6722 Loss_G: 8.0705\n",
      "[0/25][636/782] Loss_D: 2.7589 Loss_G: 3.3539\n",
      "[0/25][637/782] Loss_D: 1.1787 Loss_G: 0.9429\n",
      "[0/25][638/782] Loss_D: 1.4583 Loss_G: 6.7513\n",
      "[0/25][639/782] Loss_D: 0.3614 Loss_G: 5.9364\n",
      "[0/25][640/782] Loss_D: 0.6559 Loss_G: 2.6719\n",
      "[0/25][641/782] Loss_D: 1.1850 Loss_G: 5.6228\n",
      "[0/25][642/782] Loss_D: 0.5607 Loss_G: 4.6032\n",
      "[0/25][643/782] Loss_D: 0.5695 Loss_G: 2.7407\n",
      "[0/25][644/782] Loss_D: 0.5592 Loss_G: 4.0232\n",
      "[0/25][645/782] Loss_D: 0.3452 Loss_G: 4.3339\n",
      "[0/25][646/782] Loss_D: 0.3610 Loss_G: 3.6544\n",
      "[0/25][647/782] Loss_D: 0.4545 Loss_G: 3.7172\n",
      "[0/25][648/782] Loss_D: 0.4826 Loss_G: 3.8163\n",
      "[0/25][649/782] Loss_D: 0.6831 Loss_G: 2.6851\n",
      "[0/25][650/782] Loss_D: 0.4957 Loss_G: 5.2620\n",
      "[0/25][651/782] Loss_D: 0.1120 Loss_G: 5.5556\n",
      "[0/25][652/782] Loss_D: 0.3171 Loss_G: 3.6644\n",
      "[0/25][653/782] Loss_D: 0.2730 Loss_G: 3.5947\n",
      "[0/25][654/782] Loss_D: 0.2757 Loss_G: 4.7182\n",
      "[0/25][655/782] Loss_D: 0.3507 Loss_G: 3.2876\n",
      "[0/25][656/782] Loss_D: 0.2926 Loss_G: 3.8883\n",
      "[0/25][657/782] Loss_D: 0.4674 Loss_G: 5.0830\n",
      "[0/25][658/782] Loss_D: 0.3685 Loss_G: 3.9859\n",
      "[0/25][659/782] Loss_D: 0.3997 Loss_G: 3.6773\n",
      "[0/25][660/782] Loss_D: 0.7358 Loss_G: 6.5590\n",
      "[0/25][661/782] Loss_D: 1.1355 Loss_G: 2.7238\n",
      "[0/25][662/782] Loss_D: 0.8439 Loss_G: 7.8101\n",
      "[0/25][663/782] Loss_D: 0.8874 Loss_G: 4.6568\n",
      "[0/25][664/782] Loss_D: 0.3327 Loss_G: 4.5339\n",
      "[0/25][665/782] Loss_D: 0.2735 Loss_G: 5.9219\n",
      "[0/25][666/782] Loss_D: 0.2365 Loss_G: 5.0995\n",
      "[0/25][667/782] Loss_D: 0.2856 Loss_G: 3.6793\n",
      "[0/25][668/782] Loss_D: 0.5938 Loss_G: 7.0039\n",
      "[0/25][669/782] Loss_D: 1.1555 Loss_G: 2.5495\n",
      "[0/25][670/782] Loss_D: 1.3570 Loss_G: 9.7999\n",
      "[0/25][671/782] Loss_D: 2.0030 Loss_G: 5.6784\n",
      "[0/25][672/782] Loss_D: 0.3897 Loss_G: 3.9195\n",
      "[0/25][673/782] Loss_D: 0.5391 Loss_G: 6.8326\n",
      "[0/25][674/782] Loss_D: 0.2945 Loss_G: 5.9050\n",
      "[0/25][675/782] Loss_D: 0.2497 Loss_G: 4.2377\n",
      "[0/25][676/782] Loss_D: 0.2431 Loss_G: 4.6895\n",
      "[0/25][677/782] Loss_D: 0.2308 Loss_G: 5.3294\n",
      "[0/25][678/782] Loss_D: 0.2528 Loss_G: 4.6414\n",
      "[0/25][679/782] Loss_D: 0.4965 Loss_G: 5.4952\n",
      "[0/25][680/782] Loss_D: 0.3423 Loss_G: 4.3732\n",
      "[0/25][681/782] Loss_D: 0.6867 Loss_G: 4.5721\n",
      "[0/25][682/782] Loss_D: 0.3156 Loss_G: 4.1055\n",
      "[0/25][683/782] Loss_D: 0.4614 Loss_G: 4.4355\n",
      "[0/25][684/782] Loss_D: 0.3644 Loss_G: 4.8497\n",
      "[0/25][685/782] Loss_D: 0.3640 Loss_G: 4.1928\n",
      "[0/25][686/782] Loss_D: 0.3268 Loss_G: 4.4829\n",
      "[0/25][687/782] Loss_D: 0.3313 Loss_G: 5.0012\n",
      "[0/25][688/782] Loss_D: 0.2454 Loss_G: 5.0338\n",
      "[0/25][689/782] Loss_D: 0.2757 Loss_G: 5.4154\n",
      "[0/25][690/782] Loss_D: 0.3706 Loss_G: 3.4255\n",
      "[0/25][691/782] Loss_D: 0.4709 Loss_G: 7.4170\n",
      "[0/25][692/782] Loss_D: 0.3777 Loss_G: 4.8793\n",
      "[0/25][693/782] Loss_D: 0.2158 Loss_G: 4.6351\n",
      "[0/25][694/782] Loss_D: 0.3501 Loss_G: 6.7904\n",
      "[0/25][695/782] Loss_D: 0.2920 Loss_G: 5.0148\n",
      "[0/25][696/782] Loss_D: 0.3717 Loss_G: 5.2638\n",
      "[0/25][697/782] Loss_D: 0.2439 Loss_G: 6.2194\n",
      "[0/25][698/782] Loss_D: 0.3049 Loss_G: 5.4734\n",
      "[0/25][699/782] Loss_D: 0.5054 Loss_G: 5.3589\n",
      "[0/25][700/782] Loss_D: 0.6294 Loss_G: 5.5907\n",
      "[0/25][701/782] Loss_D: 0.3102 Loss_G: 5.9944\n",
      "[0/25][702/782] Loss_D: 0.3926 Loss_G: 6.1364\n",
      "[0/25][703/782] Loss_D: 0.2587 Loss_G: 5.6105\n",
      "[0/25][704/782] Loss_D: 0.3067 Loss_G: 5.6940\n",
      "[0/25][705/782] Loss_D: 0.1512 Loss_G: 5.5171\n",
      "[0/25][706/782] Loss_D: 0.2396 Loss_G: 7.2730\n",
      "[0/25][707/782] Loss_D: 0.0925 Loss_G: 6.7092\n",
      "[0/25][708/782] Loss_D: 0.2102 Loss_G: 6.8440\n",
      "[0/25][709/782] Loss_D: 0.1790 Loss_G: 8.0465\n",
      "[0/25][710/782] Loss_D: 0.2042 Loss_G: 6.9364\n",
      "[0/25][711/782] Loss_D: 0.2975 Loss_G: 8.3714\n",
      "[0/25][712/782] Loss_D: 0.1421 Loss_G: 7.1497\n",
      "[0/25][713/782] Loss_D: 0.1083 Loss_G: 6.5449\n",
      "[0/25][714/782] Loss_D: 0.1199 Loss_G: 7.1996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][715/782] Loss_D: 0.1500 Loss_G: 8.0604\n",
      "[0/25][716/782] Loss_D: 0.0626 Loss_G: 7.4049\n",
      "[0/25][717/782] Loss_D: 0.1345 Loss_G: 5.1880\n",
      "[0/25][718/782] Loss_D: 0.3648 Loss_G: 14.1223\n",
      "[0/25][719/782] Loss_D: 0.7964 Loss_G: 10.0186\n",
      "[0/25][720/782] Loss_D: 0.0588 Loss_G: 6.2522\n",
      "[0/25][721/782] Loss_D: 0.4088 Loss_G: 11.2832\n",
      "[0/25][722/782] Loss_D: 0.2145 Loss_G: 9.2792\n",
      "[0/25][723/782] Loss_D: 0.3685 Loss_G: 9.2177\n",
      "[0/25][724/782] Loss_D: 0.4081 Loss_G: 5.0083\n",
      "[0/25][725/782] Loss_D: 1.1106 Loss_G: 17.8044\n",
      "[0/25][726/782] Loss_D: 2.7034 Loss_G: 9.6176\n",
      "[0/25][727/782] Loss_D: 0.1281 Loss_G: 5.6482\n",
      "[0/25][728/782] Loss_D: 0.6907 Loss_G: 9.7399\n",
      "[0/25][729/782] Loss_D: 0.3317 Loss_G: 7.0894\n",
      "[0/25][730/782] Loss_D: 0.2707 Loss_G: 6.0329\n",
      "[0/25][731/782] Loss_D: 0.3102 Loss_G: 7.9529\n",
      "[0/25][732/782] Loss_D: 0.7770 Loss_G: 1.9441\n",
      "[0/25][733/782] Loss_D: 1.3143 Loss_G: 11.7156\n",
      "[0/25][734/782] Loss_D: 0.2169 Loss_G: 12.7913\n",
      "[0/25][735/782] Loss_D: 0.7791 Loss_G: 7.7936\n",
      "[0/25][736/782] Loss_D: 0.0674 Loss_G: 4.6774\n",
      "[0/25][737/782] Loss_D: 0.3560 Loss_G: 7.1047\n",
      "[0/25][738/782] Loss_D: 0.0717 Loss_G: 7.3095\n",
      "[0/25][739/782] Loss_D: 0.1480 Loss_G: 6.3478\n",
      "[0/25][740/782] Loss_D: 0.1044 Loss_G: 5.4657\n",
      "[0/25][741/782] Loss_D: 0.5582 Loss_G: 9.1023\n",
      "[0/25][742/782] Loss_D: 0.8361 Loss_G: 4.4493\n",
      "[0/25][743/782] Loss_D: 1.0022 Loss_G: 10.0590\n",
      "[0/25][744/782] Loss_D: 0.7739 Loss_G: 5.9333\n",
      "[0/25][745/782] Loss_D: 0.0825 Loss_G: 3.5334\n",
      "[0/25][746/782] Loss_D: 0.4653 Loss_G: 9.8649\n",
      "[0/25][747/782] Loss_D: 0.1126 Loss_G: 9.8792\n",
      "[0/25][748/782] Loss_D: 0.3695 Loss_G: 5.8001\n",
      "[0/25][749/782] Loss_D: 0.4665 Loss_G: 5.7554\n",
      "[0/25][750/782] Loss_D: 0.4798 Loss_G: 8.0038\n",
      "[0/25][751/782] Loss_D: 0.6128 Loss_G: 4.2003\n",
      "[0/25][752/782] Loss_D: 0.8790 Loss_G: 6.7546\n",
      "[0/25][753/782] Loss_D: 0.6448 Loss_G: 4.6075\n",
      "[0/25][754/782] Loss_D: 1.1952 Loss_G: 5.5886\n",
      "[0/25][755/782] Loss_D: 0.4289 Loss_G: 4.6655\n",
      "[0/25][756/782] Loss_D: 0.2723 Loss_G: 4.3503\n",
      "[0/25][757/782] Loss_D: 0.2001 Loss_G: 5.4932\n",
      "[0/25][758/782] Loss_D: 0.1194 Loss_G: 5.5613\n",
      "[0/25][759/782] Loss_D: 0.2177 Loss_G: 5.0008\n",
      "[0/25][760/782] Loss_D: 0.2879 Loss_G: 6.1656\n",
      "[0/25][761/782] Loss_D: 0.3101 Loss_G: 4.4877\n",
      "[0/25][762/782] Loss_D: 0.5353 Loss_G: 8.6710\n",
      "[0/25][763/782] Loss_D: 1.3956 Loss_G: 1.9799\n",
      "[0/25][764/782] Loss_D: 1.5031 Loss_G: 9.6360\n",
      "[0/25][765/782] Loss_D: 0.6697 Loss_G: 6.6889\n",
      "[0/25][766/782] Loss_D: 0.4047 Loss_G: 3.1231\n",
      "[0/25][767/782] Loss_D: 0.9782 Loss_G: 4.7586\n",
      "[0/25][768/782] Loss_D: 0.2543 Loss_G: 4.6259\n",
      "[0/25][769/782] Loss_D: 0.3426 Loss_G: 2.9708\n",
      "[0/25][770/782] Loss_D: 0.6147 Loss_G: 4.4486\n",
      "[0/25][771/782] Loss_D: 0.8317 Loss_G: 2.4465\n",
      "[0/25][772/782] Loss_D: 0.6347 Loss_G: 5.9200\n",
      "[0/25][773/782] Loss_D: 0.3152 Loss_G: 5.1405\n",
      "[0/25][774/782] Loss_D: 0.2213 Loss_G: 3.3549\n",
      "[0/25][775/782] Loss_D: 0.3018 Loss_G: 4.0003\n",
      "[0/25][776/782] Loss_D: 0.2512 Loss_G: 4.6371\n",
      "[0/25][777/782] Loss_D: 0.2571 Loss_G: 3.5714\n",
      "[0/25][778/782] Loss_D: 0.2332 Loss_G: 4.2320\n",
      "[0/25][779/782] Loss_D: 0.2894 Loss_G: 5.7587\n",
      "[0/25][780/782] Loss_D: 0.5589 Loss_G: 2.7184\n",
      "[0/25][781/782] Loss_D: 0.8231 Loss_G: 5.6365\n",
      "[1/25][0/782] Loss_D: 0.6094 Loss_G: 5.8356\n",
      "[1/25][1/782] Loss_D: 0.7497 Loss_G: 3.4837\n",
      "[1/25][2/782] Loss_D: 0.9721 Loss_G: 8.4754\n",
      "[1/25][3/782] Loss_D: 0.7455 Loss_G: 4.7452\n",
      "[1/25][4/782] Loss_D: 0.2215 Loss_G: 4.8473\n",
      "[1/25][5/782] Loss_D: 0.2417 Loss_G: 5.6368\n",
      "[1/25][6/782] Loss_D: 0.3347 Loss_G: 4.4698\n",
      "[1/25][7/782] Loss_D: 0.1676 Loss_G: 4.6513\n",
      "[1/25][8/782] Loss_D: 0.2661 Loss_G: 5.5001\n",
      "[1/25][9/782] Loss_D: 0.1716 Loss_G: 4.9568\n",
      "[1/25][10/782] Loss_D: 0.4296 Loss_G: 2.5037\n",
      "[1/25][11/782] Loss_D: 0.7117 Loss_G: 8.4733\n",
      "[1/25][12/782] Loss_D: 1.7955 Loss_G: 2.5242\n",
      "[1/25][13/782] Loss_D: 0.4724 Loss_G: 4.1878\n",
      "[1/25][14/782] Loss_D: 0.7222 Loss_G: 7.6745\n",
      "[1/25][15/782] Loss_D: 1.6305 Loss_G: 1.5713\n",
      "[1/25][16/782] Loss_D: 1.2866 Loss_G: 7.7468\n",
      "[1/25][17/782] Loss_D: 0.0921 Loss_G: 8.3711\n",
      "[1/25][18/782] Loss_D: 0.3578 Loss_G: 5.2086\n",
      "[1/25][19/782] Loss_D: 0.2556 Loss_G: 4.0285\n",
      "[1/25][20/782] Loss_D: 0.3453 Loss_G: 5.8732\n",
      "[1/25][21/782] Loss_D: 0.5963 Loss_G: 3.3993\n",
      "[1/25][22/782] Loss_D: 0.5067 Loss_G: 5.5205\n",
      "[1/25][23/782] Loss_D: 0.2845 Loss_G: 5.0180\n",
      "[1/25][24/782] Loss_D: 0.4104 Loss_G: 2.2523\n",
      "[1/25][25/782] Loss_D: 0.9836 Loss_G: 10.2490\n",
      "[1/25][26/782] Loss_D: 1.2039 Loss_G: 8.6896\n",
      "[1/25][27/782] Loss_D: 0.4584 Loss_G: 3.4611\n",
      "[1/25][28/782] Loss_D: 0.3188 Loss_G: 3.7562\n",
      "[1/25][29/782] Loss_D: 0.3692 Loss_G: 6.3051\n",
      "[1/25][30/782] Loss_D: 0.2249 Loss_G: 5.3097\n",
      "[1/25][31/782] Loss_D: 0.3214 Loss_G: 3.1273\n",
      "[1/25][32/782] Loss_D: 1.0565 Loss_G: 7.1603\n",
      "[1/25][33/782] Loss_D: 1.3617 Loss_G: 2.5455\n",
      "[1/25][34/782] Loss_D: 0.6787 Loss_G: 4.9038\n",
      "[1/25][35/782] Loss_D: 0.3957 Loss_G: 4.1620\n",
      "[1/25][36/782] Loss_D: 0.3845 Loss_G: 2.9340\n",
      "[1/25][37/782] Loss_D: 0.6070 Loss_G: 5.4613\n",
      "[1/25][38/782] Loss_D: 0.3872 Loss_G: 4.3482\n",
      "[1/25][39/782] Loss_D: 0.3749 Loss_G: 4.2805\n",
      "[1/25][40/782] Loss_D: 0.4331 Loss_G: 3.0871\n",
      "[1/25][41/782] Loss_D: 1.0077 Loss_G: 5.7556\n",
      "[1/25][42/782] Loss_D: 0.4567 Loss_G: 5.1960\n",
      "[1/25][43/782] Loss_D: 0.6618 Loss_G: 2.7709\n",
      "[1/25][44/782] Loss_D: 0.6483 Loss_G: 4.9259\n",
      "[1/25][45/782] Loss_D: 0.4816 Loss_G: 3.1078\n",
      "[1/25][46/782] Loss_D: 0.5797 Loss_G: 5.1240\n",
      "[1/25][47/782] Loss_D: 0.2895 Loss_G: 4.7906\n",
      "[1/25][48/782] Loss_D: 0.2400 Loss_G: 3.9764\n",
      "[1/25][49/782] Loss_D: 0.2629 Loss_G: 4.1013\n",
      "[1/25][50/782] Loss_D: 0.1026 Loss_G: 4.7217\n",
      "[1/25][51/782] Loss_D: 0.2547 Loss_G: 3.9177\n",
      "[1/25][52/782] Loss_D: 0.3096 Loss_G: 3.8378\n",
      "[1/25][53/782] Loss_D: 0.2249 Loss_G: 4.7749\n",
      "[1/25][54/782] Loss_D: 0.1755 Loss_G: 4.5157\n",
      "[1/25][55/782] Loss_D: 0.1955 Loss_G: 3.9821\n",
      "[1/25][56/782] Loss_D: 0.4224 Loss_G: 4.8109\n",
      "[1/25][57/782] Loss_D: 0.3539 Loss_G: 3.9219\n",
      "[1/25][58/782] Loss_D: 0.2001 Loss_G: 3.9908\n",
      "[1/25][59/782] Loss_D: 0.3538 Loss_G: 6.5437\n",
      "[1/25][60/782] Loss_D: 0.4406 Loss_G: 3.8101\n",
      "[1/25][61/782] Loss_D: 0.5453 Loss_G: 8.3715\n",
      "[1/25][62/782] Loss_D: 0.7495 Loss_G: 4.3088\n",
      "[1/25][63/782] Loss_D: 0.5621 Loss_G: 5.4641\n",
      "[1/25][64/782] Loss_D: 0.2634 Loss_G: 5.9601\n",
      "[1/25][65/782] Loss_D: 0.6918 Loss_G: 3.3619\n",
      "[1/25][66/782] Loss_D: 0.7618 Loss_G: 7.2317\n",
      "[1/25][67/782] Loss_D: 1.2158 Loss_G: 1.8526\n",
      "[1/25][68/782] Loss_D: 1.6102 Loss_G: 10.5119\n",
      "[1/25][69/782] Loss_D: 2.5830 Loss_G: 2.9931\n",
      "[1/25][70/782] Loss_D: 0.9735 Loss_G: 6.8690\n",
      "[1/25][71/782] Loss_D: 0.3483 Loss_G: 3.8097\n",
      "[1/25][72/782] Loss_D: 0.7857 Loss_G: 5.3028\n",
      "[1/25][73/782] Loss_D: 0.8659 Loss_G: 2.3233\n",
      "[1/25][74/782] Loss_D: 0.9481 Loss_G: 6.1809\n",
      "[1/25][75/782] Loss_D: 1.5273 Loss_G: 0.8018\n",
      "[1/25][76/782] Loss_D: 2.3260 Loss_G: 9.5043\n",
      "[1/25][77/782] Loss_D: 1.3727 Loss_G: 7.6647\n",
      "[1/25][78/782] Loss_D: 0.6148 Loss_G: 3.1756\n",
      "[1/25][79/782] Loss_D: 0.9230 Loss_G: 3.9932\n",
      "[1/25][80/782] Loss_D: 0.6069 Loss_G: 4.9276\n",
      "[1/25][81/782] Loss_D: 0.8384 Loss_G: 3.0421\n",
      "[1/25][82/782] Loss_D: 0.5299 Loss_G: 2.9362\n",
      "[1/25][83/782] Loss_D: 0.8420 Loss_G: 4.9943\n",
      "[1/25][84/782] Loss_D: 1.1612 Loss_G: 0.9213\n",
      "[1/25][85/782] Loss_D: 1.0357 Loss_G: 5.2674\n",
      "[1/25][86/782] Loss_D: 1.0094 Loss_G: 2.9435\n",
      "[1/25][87/782] Loss_D: 0.6142 Loss_G: 2.8492\n",
      "[1/25][88/782] Loss_D: 0.5857 Loss_G: 4.0852\n",
      "[1/25][89/782] Loss_D: 0.5801 Loss_G: 2.5179\n",
      "[1/25][90/782] Loss_D: 0.6319 Loss_G: 4.3524\n",
      "[1/25][91/782] Loss_D: 0.7360 Loss_G: 2.1630\n",
      "[1/25][92/782] Loss_D: 0.7846 Loss_G: 4.4502\n",
      "[1/25][93/782] Loss_D: 0.4008 Loss_G: 2.2159\n",
      "[1/25][94/782] Loss_D: 1.6704 Loss_G: 8.2183\n",
      "[1/25][95/782] Loss_D: 3.3143 Loss_G: 0.3892\n",
      "[1/25][96/782] Loss_D: 2.4841 Loss_G: 5.1023\n",
      "[1/25][97/782] Loss_D: 1.1166 Loss_G: 3.5116\n",
      "[1/25][98/782] Loss_D: 1.1196 Loss_G: 1.5726\n",
      "[1/25][99/782] Loss_D: 1.2406 Loss_G: 3.2922\n",
      "[1/25][100/782] Loss_D: 0.9425 Loss_G: 2.3246\n",
      "[1/25][101/782] Loss_D: 0.7892 Loss_G: 2.0828\n",
      "[1/25][102/782] Loss_D: 0.9401 Loss_G: 4.1497\n",
      "[1/25][103/782] Loss_D: 0.7061 Loss_G: 3.1913\n",
      "[1/25][104/782] Loss_D: 0.6458 Loss_G: 1.7224\n",
      "[1/25][105/782] Loss_D: 0.8853 Loss_G: 3.5247\n",
      "[1/25][106/782] Loss_D: 0.6121 Loss_G: 2.6963\n",
      "[1/25][107/782] Loss_D: 1.1814 Loss_G: 1.6816\n",
      "[1/25][108/782] Loss_D: 0.8644 Loss_G: 2.5447\n",
      "[1/25][109/782] Loss_D: 0.6180 Loss_G: 2.0985\n",
      "[1/25][110/782] Loss_D: 0.8385 Loss_G: 2.9536\n",
      "[1/25][111/782] Loss_D: 0.6449 Loss_G: 2.4638\n",
      "[1/25][112/782] Loss_D: 0.4505 Loss_G: 2.8019\n",
      "[1/25][113/782] Loss_D: 0.3742 Loss_G: 3.7238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/25][114/782] Loss_D: 0.2659 Loss_G: 3.8023\n",
      "[1/25][115/782] Loss_D: 0.4896 Loss_G: 1.9721\n",
      "[1/25][116/782] Loss_D: 0.5339 Loss_G: 3.2086\n",
      "[1/25][117/782] Loss_D: 0.3690 Loss_G: 4.6626\n",
      "[1/25][118/782] Loss_D: 0.7989 Loss_G: 2.0627\n",
      "[1/25][119/782] Loss_D: 0.5322 Loss_G: 3.0251\n",
      "[1/25][120/782] Loss_D: 0.5519 Loss_G: 3.4662\n",
      "[1/25][121/782] Loss_D: 0.5741 Loss_G: 2.6061\n",
      "[1/25][122/782] Loss_D: 0.6307 Loss_G: 4.2833\n",
      "[1/25][123/782] Loss_D: 0.3907 Loss_G: 3.9419\n",
      "[1/25][124/782] Loss_D: 0.6224 Loss_G: 2.0816\n",
      "[1/25][125/782] Loss_D: 0.9232 Loss_G: 5.8993\n",
      "[1/25][126/782] Loss_D: 0.6208 Loss_G: 3.9322\n",
      "[1/25][127/782] Loss_D: 0.2641 Loss_G: 3.9609\n",
      "[1/25][128/782] Loss_D: 0.1433 Loss_G: 4.2077\n",
      "[1/25][129/782] Loss_D: 0.3565 Loss_G: 3.5574\n",
      "[1/25][130/782] Loss_D: 0.4974 Loss_G: 4.3057\n",
      "[1/25][131/782] Loss_D: 0.6846 Loss_G: 2.5189\n",
      "[1/25][132/782] Loss_D: 0.7441 Loss_G: 4.9922\n",
      "[1/25][133/782] Loss_D: 0.8632 Loss_G: 0.7562\n",
      "[1/25][134/782] Loss_D: 2.0383 Loss_G: 10.0907\n",
      "[1/25][135/782] Loss_D: 1.7567 Loss_G: 1.7173\n",
      "[1/25][136/782] Loss_D: 1.2204 Loss_G: 4.8859\n",
      "[1/25][137/782] Loss_D: 0.4494 Loss_G: 3.9840\n",
      "[1/25][138/782] Loss_D: 0.5114 Loss_G: 2.7623\n",
      "[1/25][139/782] Loss_D: 0.6189 Loss_G: 2.7123\n",
      "[1/25][140/782] Loss_D: 0.9880 Loss_G: 3.7793\n",
      "[1/25][141/782] Loss_D: 1.5120 Loss_G: 1.2386\n",
      "[1/25][142/782] Loss_D: 1.1879 Loss_G: 4.4969\n",
      "[1/25][143/782] Loss_D: 1.1063 Loss_G: 2.4953\n",
      "[1/25][144/782] Loss_D: 1.0101 Loss_G: 3.1486\n",
      "[1/25][145/782] Loss_D: 0.7339 Loss_G: 3.1416\n",
      "[1/25][146/782] Loss_D: 0.7549 Loss_G: 2.9955\n",
      "[1/25][147/782] Loss_D: 1.0034 Loss_G: 3.4285\n",
      "[1/25][148/782] Loss_D: 0.9907 Loss_G: 4.1480\n",
      "[1/25][149/782] Loss_D: 0.7541 Loss_G: 2.7625\n",
      "[1/25][150/782] Loss_D: 0.9931 Loss_G: 3.3763\n",
      "[1/25][151/782] Loss_D: 0.5954 Loss_G: 4.3442\n",
      "[1/25][152/782] Loss_D: 0.4500 Loss_G: 3.2901\n",
      "[1/25][153/782] Loss_D: 0.3685 Loss_G: 4.0388\n",
      "[1/25][154/782] Loss_D: 0.4011 Loss_G: 3.5921\n",
      "[1/25][155/782] Loss_D: 0.5293 Loss_G: 2.5582\n",
      "[1/25][156/782] Loss_D: 0.4905 Loss_G: 5.5069\n",
      "[1/25][157/782] Loss_D: 0.5572 Loss_G: 3.3658\n",
      "[1/25][158/782] Loss_D: 0.4270 Loss_G: 4.1316\n",
      "[1/25][159/782] Loss_D: 0.4338 Loss_G: 3.6407\n",
      "[1/25][160/782] Loss_D: 0.4224 Loss_G: 3.8661\n",
      "[1/25][161/782] Loss_D: 0.6821 Loss_G: 3.4429\n",
      "[1/25][162/782] Loss_D: 0.6692 Loss_G: 4.4294\n",
      "[1/25][163/782] Loss_D: 0.4764 Loss_G: 3.2379\n",
      "[1/25][164/782] Loss_D: 0.4722 Loss_G: 5.3250\n",
      "[1/25][165/782] Loss_D: 0.6249 Loss_G: 3.4825\n",
      "[1/25][166/782] Loss_D: 0.6109 Loss_G: 6.6295\n",
      "[1/25][167/782] Loss_D: 0.5989 Loss_G: 3.2944\n",
      "[1/25][168/782] Loss_D: 0.5563 Loss_G: 5.9904\n",
      "[1/25][169/782] Loss_D: 0.4779 Loss_G: 4.1180\n",
      "[1/25][170/782] Loss_D: 0.3560 Loss_G: 4.5272\n",
      "[1/25][171/782] Loss_D: 0.3441 Loss_G: 4.2404\n",
      "[1/25][172/782] Loss_D: 0.2471 Loss_G: 5.3813\n",
      "[1/25][173/782] Loss_D: 0.2659 Loss_G: 4.1267\n",
      "[1/25][174/782] Loss_D: 0.5430 Loss_G: 5.6222\n",
      "[1/25][175/782] Loss_D: 0.2403 Loss_G: 4.4861\n",
      "[1/25][176/782] Loss_D: 0.5193 Loss_G: 2.3236\n",
      "[1/25][177/782] Loss_D: 0.7424 Loss_G: 7.0721\n",
      "[1/25][178/782] Loss_D: 0.5425 Loss_G: 5.5752\n",
      "[1/25][179/782] Loss_D: 0.2943 Loss_G: 3.0934\n",
      "[1/25][180/782] Loss_D: 0.3374 Loss_G: 4.6077\n",
      "[1/25][181/782] Loss_D: 0.1703 Loss_G: 4.5897\n",
      "[1/25][182/782] Loss_D: 0.1972 Loss_G: 4.2927\n",
      "[1/25][183/782] Loss_D: 0.2305 Loss_G: 4.2661\n",
      "[1/25][184/782] Loss_D: 0.5500 Loss_G: 5.1361\n",
      "[1/25][185/782] Loss_D: 0.6165 Loss_G: 2.9970\n",
      "[1/25][186/782] Loss_D: 0.5192 Loss_G: 6.0782\n",
      "[1/25][187/782] Loss_D: 0.3416 Loss_G: 4.7199\n",
      "[1/25][188/782] Loss_D: 0.2453 Loss_G: 4.1184\n",
      "[1/25][189/782] Loss_D: 0.3745 Loss_G: 4.8643\n",
      "[1/25][190/782] Loss_D: 0.1927 Loss_G: 4.8739\n",
      "[1/25][191/782] Loss_D: 0.2072 Loss_G: 4.6616\n",
      "[1/25][192/782] Loss_D: 0.3377 Loss_G: 4.4390\n",
      "[1/25][193/782] Loss_D: 0.3149 Loss_G: 4.0508\n",
      "[1/25][194/782] Loss_D: 0.3007 Loss_G: 5.7545\n",
      "[1/25][195/782] Loss_D: 0.2620 Loss_G: 5.3044\n",
      "[1/25][196/782] Loss_D: 0.2838 Loss_G: 3.2811\n",
      "[1/25][197/782] Loss_D: 0.4951 Loss_G: 5.6627\n",
      "[1/25][198/782] Loss_D: 0.3587 Loss_G: 3.3467\n",
      "[1/25][199/782] Loss_D: 0.5852 Loss_G: 7.8843\n",
      "[1/25][200/782] Loss_D: 0.8118 Loss_G: 1.8928\n",
      "[1/25][201/782] Loss_D: 0.9326 Loss_G: 8.5629\n",
      "[1/25][202/782] Loss_D: 0.9400 Loss_G: 4.5501\n",
      "[1/25][203/782] Loss_D: 0.1566 Loss_G: 2.5522\n",
      "[1/25][204/782] Loss_D: 0.9663 Loss_G: 6.9869\n",
      "[1/25][205/782] Loss_D: 0.9614 Loss_G: 1.6114\n",
      "[1/25][206/782] Loss_D: 0.5719 Loss_G: 5.1346\n",
      "[1/25][207/782] Loss_D: 0.6360 Loss_G: 2.2433\n",
      "[1/25][208/782] Loss_D: 0.4550 Loss_G: 4.7675\n",
      "[1/25][209/782] Loss_D: 0.2789 Loss_G: 3.5486\n",
      "[1/25][210/782] Loss_D: 0.2988 Loss_G: 3.7596\n",
      "[1/25][211/782] Loss_D: 0.2511 Loss_G: 3.5609\n",
      "[1/25][212/782] Loss_D: 0.3025 Loss_G: 4.3131\n",
      "[1/25][213/782] Loss_D: 0.8433 Loss_G: 0.7261\n",
      "[1/25][214/782] Loss_D: 1.4735 Loss_G: 8.6613\n",
      "[1/25][215/782] Loss_D: 1.9895 Loss_G: 3.3268\n",
      "[1/25][216/782] Loss_D: 0.3004 Loss_G: 2.3574\n",
      "[1/25][217/782] Loss_D: 0.9451 Loss_G: 6.1531\n",
      "[1/25][218/782] Loss_D: 0.4132 Loss_G: 5.1509\n",
      "[1/25][219/782] Loss_D: 0.2871 Loss_G: 2.7683\n",
      "[1/25][220/782] Loss_D: 0.2296 Loss_G: 3.3868\n",
      "[1/25][221/782] Loss_D: 0.3712 Loss_G: 4.6707\n",
      "[1/25][222/782] Loss_D: 0.4341 Loss_G: 2.9147\n",
      "[1/25][223/782] Loss_D: 0.3635 Loss_G: 4.9016\n",
      "[1/25][224/782] Loss_D: 0.1882 Loss_G: 4.7305\n",
      "[1/25][225/782] Loss_D: 0.3768 Loss_G: 1.5584\n",
      "[1/25][226/782] Loss_D: 1.3221 Loss_G: 6.6016\n",
      "[1/25][227/782] Loss_D: 1.2693 Loss_G: 1.3409\n",
      "[1/25][228/782] Loss_D: 1.7897 Loss_G: 5.6338\n",
      "[1/25][229/782] Loss_D: 0.9422 Loss_G: 2.9091\n",
      "[1/25][230/782] Loss_D: 0.7175 Loss_G: 3.6759\n",
      "[1/25][231/782] Loss_D: 0.5229 Loss_G: 4.2808\n",
      "[1/25][232/782] Loss_D: 0.8054 Loss_G: 1.8677\n",
      "[1/25][233/782] Loss_D: 1.5648 Loss_G: 6.4466\n",
      "[1/25][234/782] Loss_D: 1.5395 Loss_G: 1.7608\n",
      "[1/25][235/782] Loss_D: 1.1426 Loss_G: 6.2699\n",
      "[1/25][236/782] Loss_D: 0.6170 Loss_G: 4.9303\n",
      "[1/25][237/782] Loss_D: 0.3045 Loss_G: 3.3743\n",
      "[1/25][238/782] Loss_D: 0.3034 Loss_G: 4.5348\n",
      "[1/25][239/782] Loss_D: 0.3833 Loss_G: 3.8344\n",
      "[1/25][240/782] Loss_D: 0.2797 Loss_G: 4.6873\n",
      "[1/25][241/782] Loss_D: 0.2787 Loss_G: 4.0969\n",
      "[1/25][242/782] Loss_D: 0.4929 Loss_G: 3.3953\n",
      "[1/25][243/782] Loss_D: 0.5194 Loss_G: 3.0337\n",
      "[1/25][244/782] Loss_D: 0.4746 Loss_G: 4.8516\n",
      "[1/25][245/782] Loss_D: 0.4850 Loss_G: 3.8531\n",
      "[1/25][246/782] Loss_D: 0.7552 Loss_G: 3.0654\n",
      "[1/25][247/782] Loss_D: 0.2842 Loss_G: 5.0016\n",
      "[1/25][248/782] Loss_D: 0.3272 Loss_G: 4.3684\n",
      "[1/25][249/782] Loss_D: 0.5180 Loss_G: 4.5146\n",
      "[1/25][250/782] Loss_D: 0.4027 Loss_G: 5.7450\n",
      "[1/25][251/782] Loss_D: 0.3589 Loss_G: 4.1809\n",
      "[1/25][252/782] Loss_D: 0.4866 Loss_G: 5.9929\n",
      "[1/25][253/782] Loss_D: 0.2087 Loss_G: 5.4263\n",
      "[1/25][254/782] Loss_D: 0.3706 Loss_G: 5.2916\n",
      "[1/25][255/782] Loss_D: 0.3092 Loss_G: 5.6219\n",
      "[1/25][256/782] Loss_D: 0.4354 Loss_G: 5.0564\n",
      "[1/25][257/782] Loss_D: 0.6337 Loss_G: 5.2061\n",
      "[1/25][258/782] Loss_D: 1.3464 Loss_G: 2.5603\n",
      "[1/25][259/782] Loss_D: 0.8451 Loss_G: 7.0446\n",
      "[1/25][260/782] Loss_D: 0.3687 Loss_G: 6.0078\n",
      "[1/25][261/782] Loss_D: 0.2072 Loss_G: 4.3337\n",
      "[1/25][262/782] Loss_D: 0.8888 Loss_G: 7.4494\n",
      "[1/25][263/782] Loss_D: 0.9027 Loss_G: 3.7540\n",
      "[1/25][264/782] Loss_D: 0.5818 Loss_G: 4.9983\n",
      "[1/25][265/782] Loss_D: 0.4429 Loss_G: 5.5298\n",
      "[1/25][266/782] Loss_D: 0.2892 Loss_G: 4.3704\n",
      "[1/25][267/782] Loss_D: 0.4587 Loss_G: 3.2301\n",
      "[1/25][268/782] Loss_D: 0.8004 Loss_G: 10.1759\n",
      "[1/25][269/782] Loss_D: 2.5234 Loss_G: 1.9621\n",
      "[1/25][270/782] Loss_D: 0.8963 Loss_G: 6.1468\n",
      "[1/25][271/782] Loss_D: 0.5168 Loss_G: 4.9965\n",
      "[1/25][272/782] Loss_D: 0.4011 Loss_G: 2.8686\n",
      "[1/25][273/782] Loss_D: 1.0257 Loss_G: 6.2683\n",
      "[1/25][274/782] Loss_D: 0.7683 Loss_G: 4.7830\n",
      "[1/25][275/782] Loss_D: 0.3854 Loss_G: 2.7037\n",
      "[1/25][276/782] Loss_D: 0.8833 Loss_G: 6.2988\n",
      "[1/25][277/782] Loss_D: 0.5708 Loss_G: 4.0326\n",
      "[1/25][278/782] Loss_D: 0.4940 Loss_G: 2.2961\n",
      "[1/25][279/782] Loss_D: 1.0695 Loss_G: 6.7107\n",
      "[1/25][280/782] Loss_D: 1.3091 Loss_G: 2.4500\n",
      "[1/25][281/782] Loss_D: 0.6677 Loss_G: 2.8936\n",
      "[1/25][282/782] Loss_D: 0.4645 Loss_G: 5.1936\n",
      "[1/25][283/782] Loss_D: 1.1313 Loss_G: 1.7868\n",
      "[1/25][284/782] Loss_D: 0.8024 Loss_G: 4.4885\n",
      "[1/25][285/782] Loss_D: 0.4072 Loss_G: 3.7997\n",
      "[1/25][286/782] Loss_D: 0.3031 Loss_G: 3.2934\n",
      "[1/25][287/782] Loss_D: 0.4558 Loss_G: 4.4122\n",
      "[1/25][288/782] Loss_D: 0.5138 Loss_G: 2.0970\n",
      "[1/25][289/782] Loss_D: 0.6225 Loss_G: 5.2539\n",
      "[1/25][290/782] Loss_D: 0.6669 Loss_G: 2.4526\n",
      "[1/25][291/782] Loss_D: 0.3261 Loss_G: 3.1916\n",
      "[1/25][292/782] Loss_D: 0.2965 Loss_G: 4.1480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/25][293/782] Loss_D: 0.2213 Loss_G: 3.8747\n",
      "[1/25][294/782] Loss_D: 0.3337 Loss_G: 2.3063\n",
      "[1/25][295/782] Loss_D: 0.4957 Loss_G: 5.7066\n",
      "[1/25][296/782] Loss_D: 0.9647 Loss_G: 1.3003\n",
      "[1/25][297/782] Loss_D: 0.8707 Loss_G: 6.2273\n",
      "[1/25][298/782] Loss_D: 0.3827 Loss_G: 4.5053\n",
      "[1/25][299/782] Loss_D: 0.2875 Loss_G: 3.0268\n",
      "[1/25][300/782] Loss_D: 0.5408 Loss_G: 4.8396\n",
      "[1/25][301/782] Loss_D: 0.3990 Loss_G: 2.5820\n",
      "[1/25][302/782] Loss_D: 0.3955 Loss_G: 3.1475\n",
      "[1/25][303/782] Loss_D: 0.3670 Loss_G: 5.6021\n",
      "[1/25][304/782] Loss_D: 0.4942 Loss_G: 2.3650\n",
      "[1/25][305/782] Loss_D: 0.5632 Loss_G: 6.1681\n",
      "[1/25][306/782] Loss_D: 0.5114 Loss_G: 2.7955\n",
      "[1/25][307/782] Loss_D: 0.4293 Loss_G: 4.8509\n",
      "[1/25][308/782] Loss_D: 0.3589 Loss_G: 3.8705\n",
      "[1/25][309/782] Loss_D: 0.2009 Loss_G: 4.5469\n",
      "[1/25][310/782] Loss_D: 0.1339 Loss_G: 4.6000\n",
      "[1/25][311/782] Loss_D: 0.2726 Loss_G: 3.6243\n",
      "[1/25][312/782] Loss_D: 0.2860 Loss_G: 3.1289\n",
      "[1/25][313/782] Loss_D: 0.2968 Loss_G: 4.9931\n",
      "[1/25][314/782] Loss_D: 0.2117 Loss_G: 4.4998\n",
      "[1/25][315/782] Loss_D: 0.1914 Loss_G: 3.5386\n",
      "[1/25][316/782] Loss_D: 0.2303 Loss_G: 3.6511\n",
      "[1/25][317/782] Loss_D: 0.2628 Loss_G: 3.7684\n",
      "[1/25][318/782] Loss_D: 0.1944 Loss_G: 4.2341\n",
      "[1/25][319/782] Loss_D: 0.3132 Loss_G: 2.8873\n",
      "[1/25][320/782] Loss_D: 0.3599 Loss_G: 5.3711\n",
      "[1/25][321/782] Loss_D: 0.4229 Loss_G: 3.7866\n",
      "[1/25][322/782] Loss_D: 0.2362 Loss_G: 4.2460\n",
      "[1/25][323/782] Loss_D: 0.2268 Loss_G: 4.6254\n",
      "[1/25][324/782] Loss_D: 0.1744 Loss_G: 4.2126\n",
      "[1/25][325/782] Loss_D: 0.1610 Loss_G: 4.0680\n",
      "[1/25][326/782] Loss_D: 0.3350 Loss_G: 5.1111\n",
      "[1/25][327/782] Loss_D: 0.3085 Loss_G: 3.7901\n",
      "[1/25][328/782] Loss_D: 0.3280 Loss_G: 3.3550\n",
      "[1/25][329/782] Loss_D: 0.5580 Loss_G: 8.1594\n",
      "[1/25][330/782] Loss_D: 1.9364 Loss_G: 1.5096\n",
      "[1/25][331/782] Loss_D: 1.0267 Loss_G: 7.2207\n",
      "[1/25][332/782] Loss_D: 0.6156 Loss_G: 3.1395\n",
      "[1/25][333/782] Loss_D: 0.5992 Loss_G: 7.8581\n",
      "[1/25][334/782] Loss_D: 0.3471 Loss_G: 6.2505\n",
      "[1/25][335/782] Loss_D: 0.1461 Loss_G: 4.4865\n",
      "[1/25][336/782] Loss_D: 0.6067 Loss_G: 4.5798\n",
      "[1/25][337/782] Loss_D: 0.3479 Loss_G: 5.3562\n",
      "[1/25][338/782] Loss_D: 0.4393 Loss_G: 3.0132\n",
      "[1/25][339/782] Loss_D: 0.8848 Loss_G: 8.3554\n",
      "[1/25][340/782] Loss_D: 1.6909 Loss_G: 0.7376\n",
      "[1/25][341/782] Loss_D: 1.9086 Loss_G: 9.8017\n",
      "[1/25][342/782] Loss_D: 1.9659 Loss_G: 2.9764\n",
      "[1/25][343/782] Loss_D: 0.5709 Loss_G: 4.8939\n",
      "[1/25][344/782] Loss_D: 0.1954 Loss_G: 4.8079\n",
      "[1/25][345/782] Loss_D: 0.1805 Loss_G: 4.0624\n",
      "[1/25][346/782] Loss_D: 0.3276 Loss_G: 5.1845\n",
      "[1/25][347/782] Loss_D: 0.2939 Loss_G: 3.9425\n",
      "[1/25][348/782] Loss_D: 0.7107 Loss_G: 5.7788\n",
      "[1/25][349/782] Loss_D: 0.4093 Loss_G: 3.9119\n",
      "[1/25][350/782] Loss_D: 0.3034 Loss_G: 3.4692\n",
      "[1/25][351/782] Loss_D: 0.8632 Loss_G: 9.1505\n",
      "[1/25][352/782] Loss_D: 1.4807 Loss_G: 3.1280\n",
      "[1/25][353/782] Loss_D: 0.6162 Loss_G: 4.6195\n",
      "[1/25][354/782] Loss_D: 0.4450 Loss_G: 4.3372\n",
      "[1/25][355/782] Loss_D: 0.1917 Loss_G: 4.6373\n",
      "[1/25][356/782] Loss_D: 0.5546 Loss_G: 3.2061\n",
      "[1/25][357/782] Loss_D: 0.4586 Loss_G: 5.4856\n",
      "[1/25][358/782] Loss_D: 0.6581 Loss_G: 3.8058\n",
      "[1/25][359/782] Loss_D: 0.2425 Loss_G: 4.0482\n",
      "[1/25][360/782] Loss_D: 0.3521 Loss_G: 5.6095\n",
      "[1/25][361/782] Loss_D: 0.2866 Loss_G: 4.4947\n",
      "[1/25][362/782] Loss_D: 0.1578 Loss_G: 3.9523\n",
      "[1/25][363/782] Loss_D: 0.2579 Loss_G: 4.6641\n",
      "[1/25][364/782] Loss_D: 0.2363 Loss_G: 4.2534\n",
      "[1/25][365/782] Loss_D: 0.3596 Loss_G: 4.1135\n",
      "[1/25][366/782] Loss_D: 0.2151 Loss_G: 4.7594\n",
      "[1/25][367/782] Loss_D: 0.3094 Loss_G: 3.7968\n",
      "[1/25][368/782] Loss_D: 0.4489 Loss_G: 3.4352\n",
      "[1/25][369/782] Loss_D: 0.3372 Loss_G: 5.9014\n",
      "[1/25][370/782] Loss_D: 0.7842 Loss_G: 2.4297\n",
      "[1/25][371/782] Loss_D: 0.6297 Loss_G: 6.8371\n",
      "[1/25][372/782] Loss_D: 0.2847 Loss_G: 6.2181\n",
      "[1/25][373/782] Loss_D: 0.1959 Loss_G: 4.3836\n",
      "[1/25][374/782] Loss_D: 0.3814 Loss_G: 3.3778\n",
      "[1/25][375/782] Loss_D: 0.3830 Loss_G: 6.3931\n",
      "[1/25][376/782] Loss_D: 0.2414 Loss_G: 5.1791\n",
      "[1/25][377/782] Loss_D: 0.2555 Loss_G: 2.9889\n",
      "[1/25][378/782] Loss_D: 0.4602 Loss_G: 6.4900\n",
      "[1/25][379/782] Loss_D: 0.6011 Loss_G: 2.5121\n",
      "[1/25][380/782] Loss_D: 0.4916 Loss_G: 3.7334\n",
      "[1/25][381/782] Loss_D: 0.3063 Loss_G: 5.4813\n",
      "[1/25][382/782] Loss_D: 0.2254 Loss_G: 3.9696\n",
      "[1/25][383/782] Loss_D: 0.3439 Loss_G: 5.1506\n",
      "[1/25][384/782] Loss_D: 0.4198 Loss_G: 2.9271\n",
      "[1/25][385/782] Loss_D: 0.3622 Loss_G: 5.2481\n",
      "[1/25][386/782] Loss_D: 0.1957 Loss_G: 4.8929\n",
      "[1/25][387/782] Loss_D: 0.4902 Loss_G: 3.4674\n",
      "[1/25][388/782] Loss_D: 0.4546 Loss_G: 3.5544\n",
      "[1/25][389/782] Loss_D: 0.3459 Loss_G: 4.6145\n",
      "[1/25][390/782] Loss_D: 0.3816 Loss_G: 3.1921\n",
      "[1/25][391/782] Loss_D: 0.3470 Loss_G: 5.7561\n",
      "[1/25][392/782] Loss_D: 0.2169 Loss_G: 4.8513\n",
      "[1/25][393/782] Loss_D: 0.5540 Loss_G: 2.5421\n",
      "[1/25][394/782] Loss_D: 0.6999 Loss_G: 6.7593\n",
      "[1/25][395/782] Loss_D: 0.3489 Loss_G: 4.8581\n",
      "[1/25][396/782] Loss_D: 0.4325 Loss_G: 3.0230\n",
      "[1/25][397/782] Loss_D: 0.9857 Loss_G: 6.4763\n",
      "[1/25][398/782] Loss_D: 0.5483 Loss_G: 4.3847\n",
      "[1/25][399/782] Loss_D: 0.6175 Loss_G: 2.0942\n",
      "[1/25][400/782] Loss_D: 1.0313 Loss_G: 7.0306\n",
      "[1/25][401/782] Loss_D: 0.7391 Loss_G: 3.6901\n",
      "[1/25][402/782] Loss_D: 0.5163 Loss_G: 4.7182\n",
      "[1/25][403/782] Loss_D: 0.5759 Loss_G: 3.2521\n",
      "[1/25][404/782] Loss_D: 0.6848 Loss_G: 5.3841\n",
      "[1/25][405/782] Loss_D: 1.2966 Loss_G: 1.4436\n",
      "[1/25][406/782] Loss_D: 1.6236 Loss_G: 8.3397\n",
      "[1/25][407/782] Loss_D: 1.9579 Loss_G: 2.7202\n",
      "[1/25][408/782] Loss_D: 0.5951 Loss_G: 4.0239\n",
      "[1/25][409/782] Loss_D: 0.6218 Loss_G: 5.3356\n",
      "[1/25][410/782] Loss_D: 1.0804 Loss_G: 1.4870\n",
      "[1/25][411/782] Loss_D: 1.8167 Loss_G: 6.6858\n",
      "[1/25][412/782] Loss_D: 1.1165 Loss_G: 3.6738\n",
      "[1/25][413/782] Loss_D: 0.3736 Loss_G: 2.8870\n",
      "[1/25][414/782] Loss_D: 0.8650 Loss_G: 6.1054\n",
      "[1/25][415/782] Loss_D: 1.0956 Loss_G: 2.4687\n",
      "[1/25][416/782] Loss_D: 0.8124 Loss_G: 2.5384\n",
      "[1/25][417/782] Loss_D: 0.8510 Loss_G: 6.0604\n",
      "[1/25][418/782] Loss_D: 0.8105 Loss_G: 3.5537\n",
      "[1/25][419/782] Loss_D: 0.5774 Loss_G: 3.9178\n",
      "[1/25][420/782] Loss_D: 0.4565 Loss_G: 5.1082\n",
      "[1/25][421/782] Loss_D: 0.6519 Loss_G: 2.7265\n",
      "[1/25][422/782] Loss_D: 0.8905 Loss_G: 5.0377\n",
      "[1/25][423/782] Loss_D: 0.9791 Loss_G: 2.1850\n",
      "[1/25][424/782] Loss_D: 0.6729 Loss_G: 4.9403\n",
      "[1/25][425/782] Loss_D: 0.6193 Loss_G: 2.8985\n",
      "[1/25][426/782] Loss_D: 0.7439 Loss_G: 2.4369\n",
      "[1/25][427/782] Loss_D: 0.5916 Loss_G: 5.5945\n",
      "[1/25][428/782] Loss_D: 0.8435 Loss_G: 1.5722\n",
      "[1/25][429/782] Loss_D: 0.8281 Loss_G: 5.5236\n",
      "[1/25][430/782] Loss_D: 0.5719 Loss_G: 3.4236\n",
      "[1/25][431/782] Loss_D: 0.2672 Loss_G: 3.0195\n",
      "[1/25][432/782] Loss_D: 0.5138 Loss_G: 3.6252\n",
      "[1/25][433/782] Loss_D: 0.3514 Loss_G: 3.4951\n",
      "[1/25][434/782] Loss_D: 0.4714 Loss_G: 2.5025\n",
      "[1/25][435/782] Loss_D: 0.5056 Loss_G: 3.6576\n",
      "[1/25][436/782] Loss_D: 0.4482 Loss_G: 3.3403\n",
      "[1/25][437/782] Loss_D: 0.5103 Loss_G: 2.2381\n",
      "[1/25][438/782] Loss_D: 0.8138 Loss_G: 3.9801\n",
      "[1/25][439/782] Loss_D: 0.6487 Loss_G: 1.8311\n",
      "[1/25][440/782] Loss_D: 0.7669 Loss_G: 6.1708\n",
      "[1/25][441/782] Loss_D: 0.6061 Loss_G: 3.2127\n",
      "[1/25][442/782] Loss_D: 0.5182 Loss_G: 3.0840\n",
      "[1/25][443/782] Loss_D: 0.3069 Loss_G: 3.6619\n",
      "[1/25][444/782] Loss_D: 0.3487 Loss_G: 4.2903\n",
      "[1/25][445/782] Loss_D: 0.4306 Loss_G: 2.7046\n",
      "[1/25][446/782] Loss_D: 0.4193 Loss_G: 4.1040\n",
      "[1/25][447/782] Loss_D: 0.3074 Loss_G: 5.0977\n",
      "[1/25][448/782] Loss_D: 0.5068 Loss_G: 2.4792\n",
      "[1/25][449/782] Loss_D: 0.5250 Loss_G: 3.9909\n",
      "[1/25][450/782] Loss_D: 0.5427 Loss_G: 3.2739\n",
      "[1/25][451/782] Loss_D: 0.5501 Loss_G: 3.6214\n",
      "[1/25][452/782] Loss_D: 0.3641 Loss_G: 3.8060\n",
      "[1/25][453/782] Loss_D: 0.3900 Loss_G: 3.0654\n",
      "[1/25][454/782] Loss_D: 0.6834 Loss_G: 5.3408\n",
      "[1/25][455/782] Loss_D: 1.1110 Loss_G: 0.9090\n",
      "[1/25][456/782] Loss_D: 1.4069 Loss_G: 5.3286\n",
      "[1/25][457/782] Loss_D: 0.6851 Loss_G: 2.3348\n",
      "[1/25][458/782] Loss_D: 0.9353 Loss_G: 3.9233\n",
      "[1/25][459/782] Loss_D: 0.5745 Loss_G: 4.3720\n",
      "[1/25][460/782] Loss_D: 1.1732 Loss_G: 0.8134\n",
      "[1/25][461/782] Loss_D: 1.6499 Loss_G: 6.8031\n",
      "[1/25][462/782] Loss_D: 2.5072 Loss_G: 1.2844\n",
      "[1/25][463/782] Loss_D: 0.9849 Loss_G: 3.5472\n",
      "[1/25][464/782] Loss_D: 0.9352 Loss_G: 4.3840\n",
      "[1/25][465/782] Loss_D: 0.7782 Loss_G: 2.3377\n",
      "[1/25][466/782] Loss_D: 1.3201 Loss_G: 2.4232\n",
      "[1/25][467/782] Loss_D: 0.8102 Loss_G: 3.9413\n",
      "[1/25][468/782] Loss_D: 0.6857 Loss_G: 2.1288\n",
      "[1/25][469/782] Loss_D: 0.4966 Loss_G: 3.6001\n",
      "[1/25][470/782] Loss_D: 0.4387 Loss_G: 3.3529\n",
      "[1/25][471/782] Loss_D: 0.5622 Loss_G: 2.8122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/25][472/782] Loss_D: 0.6660 Loss_G: 4.1650\n",
      "[1/25][473/782] Loss_D: 0.5174 Loss_G: 2.3964\n",
      "[1/25][474/782] Loss_D: 0.3917 Loss_G: 3.4706\n",
      "[1/25][475/782] Loss_D: 0.5448 Loss_G: 4.7261\n",
      "[1/25][476/782] Loss_D: 0.9345 Loss_G: 1.5243\n",
      "[1/25][477/782] Loss_D: 0.9846 Loss_G: 4.7485\n",
      "[1/25][478/782] Loss_D: 0.7742 Loss_G: 2.6869\n",
      "[1/25][479/782] Loss_D: 0.7180 Loss_G: 4.2355\n",
      "[1/25][480/782] Loss_D: 0.5451 Loss_G: 3.0077\n",
      "[1/25][481/782] Loss_D: 0.4060 Loss_G: 4.3497\n",
      "[1/25][482/782] Loss_D: 0.4707 Loss_G: 3.1523\n",
      "[1/25][483/782] Loss_D: 0.4641 Loss_G: 3.0924\n",
      "[1/25][484/782] Loss_D: 0.6259 Loss_G: 5.4356\n",
      "[1/25][485/782] Loss_D: 0.9470 Loss_G: 2.2676\n",
      "[1/25][486/782] Loss_D: 0.8575 Loss_G: 4.0077\n",
      "[1/25][487/782] Loss_D: 0.6334 Loss_G: 4.4567\n",
      "[1/25][488/782] Loss_D: 0.9747 Loss_G: 1.9322\n",
      "[1/25][489/782] Loss_D: 0.7827 Loss_G: 5.9447\n",
      "[1/25][490/782] Loss_D: 0.5087 Loss_G: 3.8438\n",
      "[1/25][491/782] Loss_D: 0.5534 Loss_G: 2.9061\n",
      "[1/25][492/782] Loss_D: 0.4238 Loss_G: 4.7101\n",
      "[1/25][493/782] Loss_D: 0.7093 Loss_G: 2.7423\n",
      "[1/25][494/782] Loss_D: 0.8434 Loss_G: 2.7679\n",
      "[1/25][495/782] Loss_D: 0.6249 Loss_G: 4.7680\n",
      "[1/25][496/782] Loss_D: 0.8826 Loss_G: 2.2041\n",
      "[1/25][497/782] Loss_D: 0.4791 Loss_G: 3.6020\n",
      "[1/25][498/782] Loss_D: 0.4081 Loss_G: 3.2054\n",
      "[1/25][499/782] Loss_D: 0.4333 Loss_G: 3.2868\n",
      "[1/25][500/782] Loss_D: 0.5071 Loss_G: 3.2450\n",
      "[1/25][501/782] Loss_D: 0.4845 Loss_G: 4.5084\n",
      "[1/25][502/782] Loss_D: 0.4242 Loss_G: 2.7289\n",
      "[1/25][503/782] Loss_D: 0.6093 Loss_G: 5.4060\n",
      "[1/25][504/782] Loss_D: 0.4123 Loss_G: 2.8768\n",
      "[1/25][505/782] Loss_D: 0.4300 Loss_G: 4.4161\n",
      "[1/25][506/782] Loss_D: 0.4317 Loss_G: 3.0037\n",
      "[1/25][507/782] Loss_D: 0.5226 Loss_G: 4.0139\n",
      "[1/25][508/782] Loss_D: 0.3238 Loss_G: 3.4378\n",
      "[1/25][509/782] Loss_D: 0.4393 Loss_G: 2.5664\n",
      "[1/25][510/782] Loss_D: 0.5867 Loss_G: 6.2872\n",
      "[1/25][511/782] Loss_D: 1.0611 Loss_G: 1.8262\n",
      "[1/25][512/782] Loss_D: 0.8173 Loss_G: 6.2826\n",
      "[1/25][513/782] Loss_D: 0.7457 Loss_G: 2.2962\n",
      "[1/25][514/782] Loss_D: 0.8688 Loss_G: 4.4823\n",
      "[1/25][515/782] Loss_D: 0.6960 Loss_G: 2.0018\n",
      "[1/25][516/782] Loss_D: 1.2611 Loss_G: 8.3804\n",
      "[1/25][517/782] Loss_D: 2.9428 Loss_G: 0.8754\n",
      "[1/25][518/782] Loss_D: 1.5222 Loss_G: 6.7561\n",
      "[1/25][519/782] Loss_D: 0.9446 Loss_G: 3.3236\n",
      "[1/25][520/782] Loss_D: 0.5390 Loss_G: 2.4638\n",
      "[1/25][521/782] Loss_D: 0.6158 Loss_G: 4.2687\n",
      "[1/25][522/782] Loss_D: 0.5429 Loss_G: 3.5337\n",
      "[1/25][523/782] Loss_D: 0.6663 Loss_G: 3.3335\n",
      "[1/25][524/782] Loss_D: 1.2492 Loss_G: 0.6624\n",
      "[1/25][525/782] Loss_D: 2.1400 Loss_G: 6.7884\n",
      "[1/25][526/782] Loss_D: 2.3312 Loss_G: 2.2474\n",
      "[1/25][527/782] Loss_D: 0.4321 Loss_G: 1.9598\n",
      "[1/25][528/782] Loss_D: 0.7526 Loss_G: 4.2065\n",
      "[1/25][529/782] Loss_D: 0.4919 Loss_G: 3.4838\n",
      "[1/25][530/782] Loss_D: 0.2791 Loss_G: 2.9665\n",
      "[1/25][531/782] Loss_D: 0.4475 Loss_G: 3.7554\n",
      "[1/25][532/782] Loss_D: 0.3234 Loss_G: 3.7291\n",
      "[1/25][533/782] Loss_D: 0.5404 Loss_G: 2.2498\n",
      "[1/25][534/782] Loss_D: 0.6269 Loss_G: 3.6205\n",
      "[1/25][535/782] Loss_D: 0.4259 Loss_G: 3.1533\n",
      "[1/25][536/782] Loss_D: 0.4443 Loss_G: 2.8886\n",
      "[1/25][537/782] Loss_D: 0.4091 Loss_G: 3.3787\n",
      "[1/25][538/782] Loss_D: 0.4410 Loss_G: 3.3905\n",
      "[1/25][539/782] Loss_D: 0.4155 Loss_G: 3.3268\n",
      "[1/25][540/782] Loss_D: 0.3598 Loss_G: 2.9801\n",
      "[1/25][541/782] Loss_D: 0.5709 Loss_G: 3.7507\n",
      "[1/25][542/782] Loss_D: 0.5535 Loss_G: 2.7866\n",
      "[1/25][543/782] Loss_D: 0.5933 Loss_G: 4.5535\n",
      "[1/25][544/782] Loss_D: 0.3665 Loss_G: 3.7474\n",
      "[1/25][545/782] Loss_D: 0.4430 Loss_G: 2.8176\n",
      "[1/25][546/782] Loss_D: 0.3559 Loss_G: 2.8802\n",
      "[1/25][547/782] Loss_D: 0.5268 Loss_G: 4.8224\n",
      "[1/25][548/782] Loss_D: 0.5876 Loss_G: 2.6919\n",
      "[1/25][549/782] Loss_D: 0.3884 Loss_G: 3.5201\n",
      "[1/25][550/782] Loss_D: 0.4668 Loss_G: 4.2477\n",
      "[1/25][551/782] Loss_D: 0.3611 Loss_G: 3.3035\n",
      "[1/25][552/782] Loss_D: 0.4306 Loss_G: 3.7240\n",
      "[1/25][553/782] Loss_D: 0.4127 Loss_G: 3.0954\n",
      "[1/25][554/782] Loss_D: 0.5351 Loss_G: 4.4196\n",
      "[1/25][555/782] Loss_D: 0.5889 Loss_G: 2.2745\n",
      "[1/25][556/782] Loss_D: 0.7794 Loss_G: 5.6468\n",
      "[1/25][557/782] Loss_D: 0.2109 Loss_G: 5.6225\n",
      "[1/25][558/782] Loss_D: 0.5536 Loss_G: 1.6990\n",
      "[1/25][559/782] Loss_D: 1.1635 Loss_G: 6.6145\n",
      "[1/25][560/782] Loss_D: 1.0661 Loss_G: 2.6335\n",
      "[1/25][561/782] Loss_D: 0.7176 Loss_G: 5.2940\n",
      "[1/25][562/782] Loss_D: 0.6038 Loss_G: 3.5689\n",
      "[1/25][563/782] Loss_D: 0.8895 Loss_G: 3.6548\n",
      "[1/25][564/782] Loss_D: 0.5097 Loss_G: 3.3585\n",
      "[1/25][565/782] Loss_D: 0.4006 Loss_G: 3.9967\n",
      "[1/25][566/782] Loss_D: 0.4166 Loss_G: 3.0732\n",
      "[1/25][567/782] Loss_D: 0.5363 Loss_G: 3.3406\n",
      "[1/25][568/782] Loss_D: 0.5031 Loss_G: 3.7524\n",
      "[1/25][569/782] Loss_D: 0.3403 Loss_G: 3.3523\n",
      "[1/25][570/782] Loss_D: 0.2952 Loss_G: 3.0609\n",
      "[1/25][571/782] Loss_D: 0.4666 Loss_G: 4.3757\n",
      "[1/25][572/782] Loss_D: 0.5163 Loss_G: 3.7798\n",
      "[1/25][573/782] Loss_D: 0.6157 Loss_G: 1.9773\n",
      "[1/25][574/782] Loss_D: 0.6685 Loss_G: 5.2041\n",
      "[1/25][575/782] Loss_D: 0.5569 Loss_G: 3.3710\n",
      "[1/25][576/782] Loss_D: 0.3480 Loss_G: 3.5460\n",
      "[1/25][577/782] Loss_D: 0.4920 Loss_G: 2.5595\n",
      "[1/25][578/782] Loss_D: 0.3617 Loss_G: 4.2910\n",
      "[1/25][579/782] Loss_D: 0.4279 Loss_G: 2.4684\n",
      "[1/25][580/782] Loss_D: 0.6231 Loss_G: 6.3311\n",
      "[1/25][581/782] Loss_D: 1.3049 Loss_G: 0.8153\n",
      "[1/25][582/782] Loss_D: 1.5838 Loss_G: 7.2379\n",
      "[1/25][583/782] Loss_D: 1.7688 Loss_G: 0.6394\n",
      "[1/25][584/782] Loss_D: 1.6386 Loss_G: 7.0370\n",
      "[1/25][585/782] Loss_D: 0.5697 Loss_G: 5.6589\n",
      "[1/25][586/782] Loss_D: 0.5500 Loss_G: 1.8679\n",
      "[1/25][587/782] Loss_D: 1.0072 Loss_G: 4.7860\n",
      "[1/25][588/782] Loss_D: 0.4057 Loss_G: 4.3521\n",
      "[1/25][589/782] Loss_D: 0.4749 Loss_G: 2.5394\n",
      "[1/25][590/782] Loss_D: 0.6231 Loss_G: 3.5203\n",
      "[1/25][591/782] Loss_D: 0.7042 Loss_G: 2.7013\n",
      "[1/25][592/782] Loss_D: 0.6206 Loss_G: 3.4657\n",
      "[1/25][593/782] Loss_D: 0.4913 Loss_G: 2.8604\n",
      "[1/25][594/782] Loss_D: 0.3380 Loss_G: 3.6969\n",
      "[1/25][595/782] Loss_D: 0.4230 Loss_G: 2.9176\n",
      "[1/25][596/782] Loss_D: 0.4406 Loss_G: 5.0611\n",
      "[1/25][597/782] Loss_D: 0.5703 Loss_G: 2.6028\n",
      "[1/25][598/782] Loss_D: 0.6765 Loss_G: 3.6275\n",
      "[1/25][599/782] Loss_D: 0.4213 Loss_G: 3.8791\n",
      "[1/25][600/782] Loss_D: 0.4268 Loss_G: 2.8452\n",
      "[1/25][601/782] Loss_D: 0.3824 Loss_G: 3.2623\n",
      "[1/25][602/782] Loss_D: 0.2501 Loss_G: 3.9828\n",
      "[1/25][603/782] Loss_D: 0.3533 Loss_G: 3.8541\n",
      "[1/25][604/782] Loss_D: 0.3449 Loss_G: 3.4422\n",
      "[1/25][605/782] Loss_D: 0.3418 Loss_G: 3.8494\n",
      "[1/25][606/782] Loss_D: 0.4362 Loss_G: 3.5370\n",
      "[1/25][607/782] Loss_D: 0.3730 Loss_G: 4.0006\n",
      "[1/25][608/782] Loss_D: 0.5163 Loss_G: 4.6344\n",
      "[1/25][609/782] Loss_D: 0.2826 Loss_G: 4.4065\n",
      "[1/25][610/782] Loss_D: 0.6018 Loss_G: 2.1117\n",
      "[1/25][611/782] Loss_D: 0.9415 Loss_G: 7.6321\n",
      "[1/25][612/782] Loss_D: 0.8914 Loss_G: 4.2567\n",
      "[1/25][613/782] Loss_D: 0.2487 Loss_G: 3.0578\n",
      "[1/25][614/782] Loss_D: 0.6758 Loss_G: 7.2212\n",
      "[1/25][615/782] Loss_D: 0.8393 Loss_G: 3.3299\n",
      "[1/25][616/782] Loss_D: 0.4042 Loss_G: 3.4698\n",
      "[1/25][617/782] Loss_D: 0.6448 Loss_G: 6.7860\n",
      "[1/25][618/782] Loss_D: 0.9099 Loss_G: 2.6975\n",
      "[1/25][619/782] Loss_D: 0.5995 Loss_G: 6.0122\n",
      "[1/25][620/782] Loss_D: 0.3437 Loss_G: 4.1970\n",
      "[1/25][621/782] Loss_D: 0.6386 Loss_G: 1.2544\n",
      "[1/25][622/782] Loss_D: 1.3700 Loss_G: 7.7361\n",
      "[1/25][623/782] Loss_D: 0.9343 Loss_G: 2.6619\n",
      "[1/25][624/782] Loss_D: 0.3815 Loss_G: 3.8087\n",
      "[1/25][625/782] Loss_D: 0.3144 Loss_G: 5.3094\n",
      "[1/25][626/782] Loss_D: 0.6643 Loss_G: 1.9525\n",
      "[1/25][627/782] Loss_D: 1.0883 Loss_G: 7.8485\n",
      "[1/25][628/782] Loss_D: 1.9651 Loss_G: 2.0103\n",
      "[1/25][629/782] Loss_D: 0.6847 Loss_G: 3.9962\n",
      "[1/25][630/782] Loss_D: 0.5563 Loss_G: 4.6702\n",
      "[1/25][631/782] Loss_D: 0.7581 Loss_G: 2.3832\n",
      "[1/25][632/782] Loss_D: 1.1740 Loss_G: 6.1855\n",
      "[1/25][633/782] Loss_D: 2.1030 Loss_G: 0.9624\n",
      "[1/25][634/782] Loss_D: 1.5099 Loss_G: 5.5802\n",
      "[1/25][635/782] Loss_D: 0.7033 Loss_G: 4.0400\n",
      "[1/25][636/782] Loss_D: 0.3158 Loss_G: 2.7124\n",
      "[1/25][637/782] Loss_D: 0.8091 Loss_G: 4.6579\n",
      "[1/25][638/782] Loss_D: 0.3794 Loss_G: 3.9355\n",
      "[1/25][639/782] Loss_D: 0.3790 Loss_G: 2.5736\n",
      "[1/25][640/782] Loss_D: 0.7888 Loss_G: 4.3291\n",
      "[1/25][641/782] Loss_D: 0.6339 Loss_G: 3.1354\n",
      "[1/25][642/782] Loss_D: 0.5120 Loss_G: 2.5654\n",
      "[1/25][643/782] Loss_D: 0.5422 Loss_G: 3.8440\n",
      "[1/25][644/782] Loss_D: 0.5437 Loss_G: 2.8383\n",
      "[1/25][645/782] Loss_D: 0.3935 Loss_G: 3.0793\n",
      "[1/25][646/782] Loss_D: 0.4823 Loss_G: 4.0129\n",
      "[1/25][647/782] Loss_D: 0.4220 Loss_G: 3.0656\n",
      "[1/25][648/782] Loss_D: 0.3769 Loss_G: 3.3232\n",
      "[1/25][649/782] Loss_D: 0.4708 Loss_G: 3.9062\n",
      "[1/25][650/782] Loss_D: 0.3720 Loss_G: 3.1414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/25][651/782] Loss_D: 0.3831 Loss_G: 3.2373\n",
      "[1/25][652/782] Loss_D: 0.4118 Loss_G: 4.1651\n",
      "[1/25][653/782] Loss_D: 0.2344 Loss_G: 4.2699\n",
      "[1/25][654/782] Loss_D: 0.3635 Loss_G: 2.7671\n",
      "[1/25][655/782] Loss_D: 0.4129 Loss_G: 3.8056\n",
      "[1/25][656/782] Loss_D: 0.4869 Loss_G: 4.2416\n",
      "[1/25][657/782] Loss_D: 0.4219 Loss_G: 3.0658\n",
      "[1/25][658/782] Loss_D: 0.3711 Loss_G: 4.4479\n",
      "[1/25][659/782] Loss_D: 0.4338 Loss_G: 3.4128\n",
      "[1/25][660/782] Loss_D: 0.3883 Loss_G: 3.4813\n",
      "[1/25][661/782] Loss_D: 0.5184 Loss_G: 4.4233\n",
      "[1/25][662/782] Loss_D: 0.4669 Loss_G: 2.3879\n",
      "[1/25][663/782] Loss_D: 0.5817 Loss_G: 5.9557\n",
      "[1/25][664/782] Loss_D: 0.5075 Loss_G: 3.4927\n",
      "[1/25][665/782] Loss_D: 0.2561 Loss_G: 3.6444\n",
      "[1/25][666/782] Loss_D: 0.5114 Loss_G: 6.3337\n",
      "[1/25][667/782] Loss_D: 0.6116 Loss_G: 2.5372\n",
      "[1/25][668/782] Loss_D: 0.4697 Loss_G: 4.4447\n",
      "[1/25][669/782] Loss_D: 0.2946 Loss_G: 3.9596\n",
      "[1/25][670/782] Loss_D: 0.5099 Loss_G: 2.4285\n",
      "[1/25][671/782] Loss_D: 0.6318 Loss_G: 6.1365\n",
      "[1/25][672/782] Loss_D: 0.6113 Loss_G: 3.7461\n",
      "[1/25][673/782] Loss_D: 0.2808 Loss_G: 3.5301\n",
      "[1/25][674/782] Loss_D: 0.2469 Loss_G: 4.9327\n",
      "[1/25][675/782] Loss_D: 0.3613 Loss_G: 2.9104\n",
      "[1/25][676/782] Loss_D: 0.3426 Loss_G: 4.2125\n",
      "[1/25][677/782] Loss_D: 0.2687 Loss_G: 4.3551\n",
      "[1/25][678/782] Loss_D: 0.2323 Loss_G: 3.6118\n",
      "[1/25][679/782] Loss_D: 0.4927 Loss_G: 3.2830\n",
      "[1/25][680/782] Loss_D: 0.4289 Loss_G: 5.3257\n",
      "[1/25][681/782] Loss_D: 0.9404 Loss_G: 0.8683\n",
      "[1/25][682/782] Loss_D: 1.3785 Loss_G: 7.9551\n",
      "[1/25][683/782] Loss_D: 1.9700 Loss_G: 0.8533\n",
      "[1/25][684/782] Loss_D: 1.7147 Loss_G: 7.2412\n",
      "[1/25][685/782] Loss_D: 1.1746 Loss_G: 2.9431\n",
      "[1/25][686/782] Loss_D: 0.3923 Loss_G: 2.7379\n",
      "[1/25][687/782] Loss_D: 0.5459 Loss_G: 5.4100\n",
      "[1/25][688/782] Loss_D: 0.5739 Loss_G: 3.2133\n",
      "[1/25][689/782] Loss_D: 0.5036 Loss_G: 3.7207\n",
      "[1/25][690/782] Loss_D: 0.5806 Loss_G: 5.0274\n",
      "[1/25][691/782] Loss_D: 0.9542 Loss_G: 1.1604\n",
      "[1/25][692/782] Loss_D: 1.4737 Loss_G: 6.9118\n",
      "[1/25][693/782] Loss_D: 1.7435 Loss_G: 2.9422\n",
      "[1/25][694/782] Loss_D: 0.4684 Loss_G: 2.9116\n",
      "[1/25][695/782] Loss_D: 0.5387 Loss_G: 4.8490\n",
      "[1/25][696/782] Loss_D: 0.2946 Loss_G: 4.4169\n",
      "[1/25][697/782] Loss_D: 0.3469 Loss_G: 3.0727\n",
      "[1/25][698/782] Loss_D: 0.3721 Loss_G: 3.7068\n",
      "[1/25][699/782] Loss_D: 0.4086 Loss_G: 4.2870\n",
      "[1/25][700/782] Loss_D: 0.4247 Loss_G: 3.1366\n",
      "[1/25][701/782] Loss_D: 0.3626 Loss_G: 3.5557\n",
      "[1/25][702/782] Loss_D: 0.2209 Loss_G: 4.3687\n",
      "[1/25][703/782] Loss_D: 0.1445 Loss_G: 4.3234\n",
      "[1/25][704/782] Loss_D: 0.3718 Loss_G: 3.4665\n",
      "[1/25][705/782] Loss_D: 0.1850 Loss_G: 3.8104\n",
      "[1/25][706/782] Loss_D: 0.2799 Loss_G: 3.3283\n",
      "[1/25][707/782] Loss_D: 0.6427 Loss_G: 4.7924\n",
      "[1/25][708/782] Loss_D: 0.8539 Loss_G: 1.8969\n",
      "[1/25][709/782] Loss_D: 0.7898 Loss_G: 4.7109\n",
      "[1/25][710/782] Loss_D: 0.3735 Loss_G: 4.0899\n",
      "[1/25][711/782] Loss_D: 0.2615 Loss_G: 3.6632\n",
      "[1/25][712/782] Loss_D: 0.3395 Loss_G: 3.1627\n",
      "[1/25][713/782] Loss_D: 0.4923 Loss_G: 4.6185\n",
      "[1/25][714/782] Loss_D: 0.5115 Loss_G: 2.7938\n",
      "[1/25][715/782] Loss_D: 0.4728 Loss_G: 3.2690\n",
      "[1/25][716/782] Loss_D: 0.3462 Loss_G: 4.2236\n",
      "[1/25][717/782] Loss_D: 0.4571 Loss_G: 2.7697\n",
      "[1/25][718/782] Loss_D: 0.4297 Loss_G: 4.3802\n",
      "[1/25][719/782] Loss_D: 0.3319 Loss_G: 3.4592\n",
      "[1/25][720/782] Loss_D: 0.3036 Loss_G: 4.5070\n",
      "[1/25][721/782] Loss_D: 0.6187 Loss_G: 1.9204\n",
      "[1/25][722/782] Loss_D: 0.6439 Loss_G: 5.6070\n",
      "[1/25][723/782] Loss_D: 0.4323 Loss_G: 3.4539\n",
      "[1/25][724/782] Loss_D: 0.2186 Loss_G: 3.8708\n",
      "[1/25][725/782] Loss_D: 0.4432 Loss_G: 3.6036\n",
      "[1/25][726/782] Loss_D: 0.6127 Loss_G: 3.1861\n",
      "[1/25][727/782] Loss_D: 0.8489 Loss_G: 4.3975\n",
      "[1/25][728/782] Loss_D: 0.6081 Loss_G: 1.7251\n",
      "[1/25][729/782] Loss_D: 0.9094 Loss_G: 5.3695\n",
      "[1/25][730/782] Loss_D: 0.9020 Loss_G: 2.2948\n",
      "[1/25][731/782] Loss_D: 0.9823 Loss_G: 5.5570\n",
      "[1/25][732/782] Loss_D: 0.8194 Loss_G: 2.4554\n",
      "[1/25][733/782] Loss_D: 0.6198 Loss_G: 4.0039\n",
      "[1/25][734/782] Loss_D: 0.5424 Loss_G: 2.3353\n",
      "[1/25][735/782] Loss_D: 1.0185 Loss_G: 7.6046\n",
      "[1/25][736/782] Loss_D: 1.6509 Loss_G: 1.7679\n",
      "[1/25][737/782] Loss_D: 0.7261 Loss_G: 3.9182\n",
      "[1/25][738/782] Loss_D: 0.4748 Loss_G: 4.1196\n",
      "[1/25][739/782] Loss_D: 0.5271 Loss_G: 3.0923\n",
      "[1/25][740/782] Loss_D: 0.6283 Loss_G: 4.3334\n",
      "[1/25][741/782] Loss_D: 0.4141 Loss_G: 3.4722\n",
      "[1/25][742/782] Loss_D: 0.5978 Loss_G: 3.4093\n",
      "[1/25][743/782] Loss_D: 0.3653 Loss_G: 4.0764\n",
      "[1/25][744/782] Loss_D: 0.2647 Loss_G: 4.6419\n",
      "[1/25][745/782] Loss_D: 0.3655 Loss_G: 3.2098\n",
      "[1/25][746/782] Loss_D: 0.4353 Loss_G: 3.9072\n",
      "[1/25][747/782] Loss_D: 0.3145 Loss_G: 3.9047\n",
      "[1/25][748/782] Loss_D: 0.2190 Loss_G: 4.0798\n",
      "[1/25][749/782] Loss_D: 0.6685 Loss_G: 2.5505\n",
      "[1/25][750/782] Loss_D: 0.5329 Loss_G: 5.0384\n",
      "[1/25][751/782] Loss_D: 0.5418 Loss_G: 3.4621\n",
      "[1/25][752/782] Loss_D: 0.6098 Loss_G: 4.6678\n",
      "[1/25][753/782] Loss_D: 0.5908 Loss_G: 3.6646\n",
      "[1/25][754/782] Loss_D: 1.3079 Loss_G: 3.9189\n",
      "[1/25][755/782] Loss_D: 0.5589 Loss_G: 3.6426\n",
      "[1/25][756/782] Loss_D: 0.5025 Loss_G: 5.3410\n",
      "[1/25][757/782] Loss_D: 0.6041 Loss_G: 2.7574\n",
      "[1/25][758/782] Loss_D: 0.6696 Loss_G: 5.4374\n",
      "[1/25][759/782] Loss_D: 0.2616 Loss_G: 4.6148\n",
      "[1/25][760/782] Loss_D: 0.3001 Loss_G: 3.3767\n",
      "[1/25][761/782] Loss_D: 0.3583 Loss_G: 4.5165\n",
      "[1/25][762/782] Loss_D: 0.3772 Loss_G: 3.4906\n",
      "[1/25][763/782] Loss_D: 0.3820 Loss_G: 4.3131\n",
      "[1/25][764/782] Loss_D: 0.7265 Loss_G: 2.9568\n",
      "[1/25][765/782] Loss_D: 0.9440 Loss_G: 2.5041\n",
      "[1/25][766/782] Loss_D: 0.8050 Loss_G: 5.4774\n",
      "[1/25][767/782] Loss_D: 0.8708 Loss_G: 1.5482\n",
      "[1/25][768/782] Loss_D: 1.2917 Loss_G: 8.6826\n",
      "[1/25][769/782] Loss_D: 2.7929 Loss_G: 0.9348\n",
      "[1/25][770/782] Loss_D: 1.7895 Loss_G: 6.4410\n",
      "[1/25][771/782] Loss_D: 0.8738 Loss_G: 2.0082\n",
      "[1/25][772/782] Loss_D: 0.9373 Loss_G: 5.2450\n",
      "[1/25][773/782] Loss_D: 0.6133 Loss_G: 3.1055\n",
      "[1/25][774/782] Loss_D: 0.5840 Loss_G: 3.4579\n",
      "[1/25][775/782] Loss_D: 0.4290 Loss_G: 5.2040\n",
      "[1/25][776/782] Loss_D: 0.8127 Loss_G: 1.5696\n",
      "[1/25][777/782] Loss_D: 0.8105 Loss_G: 5.2220\n",
      "[1/25][778/782] Loss_D: 0.3889 Loss_G: 3.9454\n",
      "[1/25][779/782] Loss_D: 1.0681 Loss_G: 0.7017\n",
      "[1/25][780/782] Loss_D: 2.2295 Loss_G: 7.2691\n",
      "[1/25][781/782] Loss_D: 0.7586 Loss_G: 6.2907\n",
      "[2/25][0/782] Loss_D: 0.7209 Loss_G: 1.5109\n",
      "[2/25][1/782] Loss_D: 1.1018 Loss_G: 4.5446\n",
      "[2/25][2/782] Loss_D: 0.2979 Loss_G: 4.8709\n",
      "[2/25][3/782] Loss_D: 0.4710 Loss_G: 2.8040\n",
      "[2/25][4/782] Loss_D: 0.4498 Loss_G: 3.3654\n",
      "[2/25][5/782] Loss_D: 0.4321 Loss_G: 3.4724\n",
      "[2/25][6/782] Loss_D: 0.4860 Loss_G: 4.2357\n",
      "[2/25][7/782] Loss_D: 0.5550 Loss_G: 2.5609\n",
      "[2/25][8/782] Loss_D: 0.7631 Loss_G: 2.5986\n",
      "[2/25][9/782] Loss_D: 1.1310 Loss_G: 2.5789\n",
      "[2/25][10/782] Loss_D: 0.5822 Loss_G: 3.1941\n",
      "[2/25][11/782] Loss_D: 0.6129 Loss_G: 3.5341\n",
      "[2/25][12/782] Loss_D: 0.7292 Loss_G: 1.9479\n",
      "[2/25][13/782] Loss_D: 0.6618 Loss_G: 4.7937\n",
      "[2/25][14/782] Loss_D: 0.5366 Loss_G: 2.8958\n",
      "[2/25][15/782] Loss_D: 0.4611 Loss_G: 4.7573\n",
      "[2/25][16/782] Loss_D: 0.9127 Loss_G: 1.5157\n",
      "[2/25][17/782] Loss_D: 1.0355 Loss_G: 6.0396\n",
      "[2/25][18/782] Loss_D: 0.5992 Loss_G: 4.3668\n",
      "[2/25][19/782] Loss_D: 0.4710 Loss_G: 1.7390\n",
      "[2/25][20/782] Loss_D: 1.1112 Loss_G: 6.2940\n",
      "[2/25][21/782] Loss_D: 0.7470 Loss_G: 2.5811\n",
      "[2/25][22/782] Loss_D: 0.3501 Loss_G: 4.1358\n",
      "[2/25][23/782] Loss_D: 0.2359 Loss_G: 5.1340\n",
      "[2/25][24/782] Loss_D: 0.2585 Loss_G: 3.8467\n",
      "[2/25][25/782] Loss_D: 0.2846 Loss_G: 3.8489\n",
      "[2/25][26/782] Loss_D: 0.5546 Loss_G: 4.5281\n",
      "[2/25][27/782] Loss_D: 0.6480 Loss_G: 2.1660\n",
      "[2/25][28/782] Loss_D: 0.8483 Loss_G: 4.7182\n",
      "[2/25][29/782] Loss_D: 0.5161 Loss_G: 3.2758\n",
      "[2/25][30/782] Loss_D: 0.6247 Loss_G: 2.7861\n",
      "[2/25][31/782] Loss_D: 0.7366 Loss_G: 5.6374\n",
      "[2/25][32/782] Loss_D: 0.8935 Loss_G: 2.7302\n",
      "[2/25][33/782] Loss_D: 0.8135 Loss_G: 3.8715\n",
      "[2/25][34/782] Loss_D: 0.5842 Loss_G: 3.6319\n",
      "[2/25][35/782] Loss_D: 0.2755 Loss_G: 4.1115\n",
      "[2/25][36/782] Loss_D: 0.6183 Loss_G: 3.1861\n",
      "[2/25][37/782] Loss_D: 0.6113 Loss_G: 2.9902\n",
      "[2/25][38/782] Loss_D: 0.4932 Loss_G: 4.1685\n",
      "[2/25][39/782] Loss_D: 0.4406 Loss_G: 3.5857\n",
      "[2/25][40/782] Loss_D: 0.5133 Loss_G: 3.4987\n",
      "[2/25][41/782] Loss_D: 0.4025 Loss_G: 3.1742\n",
      "[2/25][42/782] Loss_D: 0.5054 Loss_G: 2.6376\n",
      "[2/25][43/782] Loss_D: 0.5050 Loss_G: 4.4729\n",
      "[2/25][44/782] Loss_D: 0.5277 Loss_G: 2.7009\n",
      "[2/25][45/782] Loss_D: 0.5319 Loss_G: 4.1682\n",
      "[2/25][46/782] Loss_D: 0.3258 Loss_G: 3.9002\n",
      "[2/25][47/782] Loss_D: 0.2935 Loss_G: 3.4775\n",
      "[2/25][48/782] Loss_D: 0.4319 Loss_G: 3.0152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/25][49/782] Loss_D: 0.4859 Loss_G: 3.5061\n",
      "[2/25][50/782] Loss_D: 0.4954 Loss_G: 2.9230\n",
      "[2/25][51/782] Loss_D: 0.3962 Loss_G: 4.1482\n",
      "[2/25][52/782] Loss_D: 0.5790 Loss_G: 2.6620\n",
      "[2/25][53/782] Loss_D: 0.4964 Loss_G: 5.1774\n",
      "[2/25][54/782] Loss_D: 0.4136 Loss_G: 3.3630\n",
      "[2/25][55/782] Loss_D: 0.3234 Loss_G: 4.4399\n",
      "[2/25][56/782] Loss_D: 0.2630 Loss_G: 3.8763\n",
      "[2/25][57/782] Loss_D: 0.2412 Loss_G: 3.1286\n",
      "[2/25][58/782] Loss_D: 0.2924 Loss_G: 4.3223\n",
      "[2/25][59/782] Loss_D: 0.5561 Loss_G: 2.9898\n",
      "[2/25][60/782] Loss_D: 0.3263 Loss_G: 4.3646\n",
      "[2/25][61/782] Loss_D: 0.3450 Loss_G: 2.7162\n",
      "[2/25][62/782] Loss_D: 0.4023 Loss_G: 4.7259\n",
      "[2/25][63/782] Loss_D: 0.3857 Loss_G: 3.0197\n",
      "[2/25][64/782] Loss_D: 0.2609 Loss_G: 3.4732\n",
      "[2/25][65/782] Loss_D: 0.3509 Loss_G: 3.9738\n",
      "[2/25][66/782] Loss_D: 0.4973 Loss_G: 2.1280\n",
      "[2/25][67/782] Loss_D: 0.5594 Loss_G: 6.1877\n",
      "[2/25][68/782] Loss_D: 0.7968 Loss_G: 2.0112\n",
      "[2/25][69/782] Loss_D: 0.4282 Loss_G: 4.0394\n",
      "[2/25][70/782] Loss_D: 0.2163 Loss_G: 4.6534\n",
      "[2/25][71/782] Loss_D: 0.3159 Loss_G: 3.3127\n",
      "[2/25][72/782] Loss_D: 0.3357 Loss_G: 3.2510\n",
      "[2/25][73/782] Loss_D: 0.2874 Loss_G: 4.0110\n",
      "[2/25][74/782] Loss_D: 0.3837 Loss_G: 2.5835\n",
      "[2/25][75/782] Loss_D: 0.3972 Loss_G: 4.6394\n",
      "[2/25][76/782] Loss_D: 0.4929 Loss_G: 2.9048\n",
      "[2/25][77/782] Loss_D: 0.4040 Loss_G: 2.8624\n",
      "[2/25][78/782] Loss_D: 0.3654 Loss_G: 4.5317\n",
      "[2/25][79/782] Loss_D: 0.3940 Loss_G: 4.4323\n",
      "[2/25][80/782] Loss_D: 0.5455 Loss_G: 1.7916\n",
      "[2/25][81/782] Loss_D: 1.0844 Loss_G: 6.9013\n",
      "[2/25][82/782] Loss_D: 1.4627 Loss_G: 1.2069\n",
      "[2/25][83/782] Loss_D: 1.5564 Loss_G: 8.1136\n",
      "[2/25][84/782] Loss_D: 1.0490 Loss_G: 4.0765\n",
      "[2/25][85/782] Loss_D: 0.5230 Loss_G: 2.5230\n",
      "[2/25][86/782] Loss_D: 0.6895 Loss_G: 6.2665\n",
      "[2/25][87/782] Loss_D: 0.7574 Loss_G: 2.9034\n",
      "[2/25][88/782] Loss_D: 0.5978 Loss_G: 4.1744\n",
      "[2/25][89/782] Loss_D: 0.4892 Loss_G: 4.7560\n",
      "[2/25][90/782] Loss_D: 0.5158 Loss_G: 3.7223\n",
      "[2/25][91/782] Loss_D: 0.4450 Loss_G: 3.7971\n",
      "[2/25][92/782] Loss_D: 0.4140 Loss_G: 3.9312\n",
      "[2/25][93/782] Loss_D: 0.5738 Loss_G: 3.2886\n",
      "[2/25][94/782] Loss_D: 0.8811 Loss_G: 4.1088\n",
      "[2/25][95/782] Loss_D: 0.3735 Loss_G: 3.9798\n",
      "[2/25][96/782] Loss_D: 0.4953 Loss_G: 3.6123\n",
      "[2/25][97/782] Loss_D: 0.4338 Loss_G: 4.2574\n",
      "[2/25][98/782] Loss_D: 1.2112 Loss_G: 1.1295\n",
      "[2/25][99/782] Loss_D: 1.7250 Loss_G: 7.8648\n",
      "[2/25][100/782] Loss_D: 1.6875 Loss_G: 3.2065\n",
      "[2/25][101/782] Loss_D: 0.2254 Loss_G: 2.5679\n",
      "[2/25][102/782] Loss_D: 0.6834 Loss_G: 5.1509\n",
      "[2/25][103/782] Loss_D: 0.6339 Loss_G: 4.0756\n",
      "[2/25][104/782] Loss_D: 0.5079 Loss_G: 2.5627\n",
      "[2/25][105/782] Loss_D: 1.1789 Loss_G: 6.0775\n",
      "[2/25][106/782] Loss_D: 0.8053 Loss_G: 3.1209\n",
      "[2/25][107/782] Loss_D: 0.7649 Loss_G: 4.8150\n",
      "[2/25][108/782] Loss_D: 0.6907 Loss_G: 2.5841\n",
      "[2/25][109/782] Loss_D: 0.5685 Loss_G: 4.5440\n",
      "[2/25][110/782] Loss_D: 0.5387 Loss_G: 3.2192\n",
      "[2/25][111/782] Loss_D: 0.4829 Loss_G: 3.3723\n",
      "[2/25][112/782] Loss_D: 0.5647 Loss_G: 4.1348\n",
      "[2/25][113/782] Loss_D: 0.7626 Loss_G: 1.9976\n",
      "[2/25][114/782] Loss_D: 0.8696 Loss_G: 5.2098\n",
      "[2/25][115/782] Loss_D: 0.6198 Loss_G: 3.0690\n",
      "[2/25][116/782] Loss_D: 0.8582 Loss_G: 3.3937\n",
      "[2/25][117/782] Loss_D: 0.6055 Loss_G: 3.2399\n",
      "[2/25][118/782] Loss_D: 1.0042 Loss_G: 4.1580\n",
      "[2/25][119/782] Loss_D: 1.0571 Loss_G: 2.2685\n",
      "[2/25][120/782] Loss_D: 1.1889 Loss_G: 5.3636\n",
      "[2/25][121/782] Loss_D: 0.6200 Loss_G: 3.2917\n",
      "[2/25][122/782] Loss_D: 0.6094 Loss_G: 6.0672\n",
      "[2/25][123/782] Loss_D: 0.7157 Loss_G: 1.8589\n",
      "[2/25][124/782] Loss_D: 0.8914 Loss_G: 7.4264\n",
      "[2/25][125/782] Loss_D: 0.4940 Loss_G: 4.9825\n",
      "[2/25][126/782] Loss_D: 0.7184 Loss_G: 0.9943\n",
      "[2/25][127/782] Loss_D: 1.6087 Loss_G: 7.9550\n",
      "[2/25][128/782] Loss_D: 0.6918 Loss_G: 4.2163\n",
      "[2/25][129/782] Loss_D: 0.4577 Loss_G: 2.0643\n",
      "[2/25][130/782] Loss_D: 1.4076 Loss_G: 7.7325\n",
      "[2/25][131/782] Loss_D: 2.0848 Loss_G: 2.2835\n",
      "[2/25][132/782] Loss_D: 0.6894 Loss_G: 3.3206\n",
      "[2/25][133/782] Loss_D: 0.4910 Loss_G: 4.2608\n",
      "[2/25][134/782] Loss_D: 0.8473 Loss_G: 2.6757\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(25):\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        # 1st Step: Updating the weights of the neural network of the discriminator\n",
    "\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # Training the discriminator with a real image of the dataset\n",
    "        real, _ = data\n",
    "        input = Variable(real)\n",
    "        target = Variable(torch.ones(input.size()[0]))\n",
    "        output = netD(input)\n",
    "        errD_real = criterion(output, target)\n",
    "        \n",
    "        # Training the discriminator with a fake image generated by the generator\n",
    "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n",
    "        fake = netG(noise)\n",
    "        target = Variable(torch.zeros(input.size()[0]))\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, target)\n",
    "        \n",
    "        # Backpropagating the total error\n",
    "        errD = errD_real + errD_fake\n",
    "        errD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # 2nd Step: Updating the weights of the neural network of the generator\n",
    "\n",
    "        netG.zero_grad()\n",
    "        target = Variable(torch.ones(input.size()[0]))\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, target)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.data[0], errG.data[0]))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True)\n",
    "            fake = netG(noise)\n",
    "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
